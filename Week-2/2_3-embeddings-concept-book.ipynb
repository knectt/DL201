{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Week 2: Embeddings Concept Book","metadata":{}},{"cell_type":"markdown","source":"# Introduction\n<hr style=\"border:2px solid gray\"> </hr>\n","metadata":{}},{"cell_type":"markdown","source":"In the previous notebook, we learned about categorical variables and the challenge to put them into AI models. Here, we work with the human languages, made of dozens of thousands of different tokens. This problem seems insurmountable, because we are no longer dealing with a small amount of categories anymore.\n\nHere, as we did with the latent factors in the recommender systems, we represent each word as a large vector. Not only does this solve the problem of encoding, but also this is creating very meaningful coordinates, enable to represent the similarities between words, finding homonyms, synonyms, and much more !","metadata":{}},{"cell_type":"markdown","source":"### Goals and Objectives\n<hr style=\"border:2px solid gray\"> </hr>\n","metadata":{}},{"cell_type":"markdown","source":"* Have an appreciation on the way words can be represented as vectors \n* Grasp the concept behind the distribution hypothesis\n* See the potential of embeddings","metadata":{}},{"cell_type":"markdown","source":"### Key Ideas\n<hr style=\"border:2px solid gray\"> </hr>\n\n* Vector Encoding\n* Feature Space\n* Distribution Hypothesis\n* Dimensionality Reduction","metadata":{}},{"cell_type":"code","source":"is_kaggle = True   # True if you are on Kaggle, False for local Windows, Linux or Mac environments.","metadata":{"execution":{"iopub.status.busy":"2022-07-20T14:14:40.726682Z","iopub.execute_input":"2022-07-20T14:14:40.727146Z","iopub.status.idle":"2022-07-20T14:14:40.761233Z","shell.execute_reply.started":"2022-07-20T14:14:40.727043Z","shell.execute_reply":"2022-07-20T14:14:40.760272Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# libraries installation\nif is_kaggle:\n    !pip install -Uqqq spaCy \n    !python -m spacy download en_core_web_lg\n    from IPython.display import clear_output\n    clear_output()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-07-20T14:14:40.762874Z","iopub.execute_input":"2022-07-20T14:14:40.763217Z","iopub.status.idle":"2022-07-20T14:17:08.057479Z","shell.execute_reply.started":"2022-07-20T14:14:40.763163Z","shell.execute_reply":"2022-07-20T14:17:08.056081Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'DL201'...\nremote: Enumerating objects: 1107, done.\u001b[K\nremote: Counting objects: 100% (55/55), done.\u001b[K\nremote: Compressing objects: 100% (43/43), done.\u001b[K\nremote: Total 1107 (delta 25), reused 27 (delta 12), pack-reused 1052\u001b[K\nReceiving objects: 100% (1107/1107), 97.57 MiB | 26.31 MiB/s, done.\nResolving deltas: 100% (649/649), done.\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nen-core-web-sm 2.3.1 requires spacy<2.4.0,>=2.3.0, but you have spacy 3.4.0 which is incompatible.\nen-core-web-lg 2.3.1 requires spacy<2.4.0,>=2.3.0, but you have spacy 3.4.0 which is incompatible.\nallennlp 2.9.3 requires spacy<3.3,>=2.1.0, but you have spacy 3.4.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mCollecting en-core-web-lg==3.4.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.4.0/en_core_web_lg-3.4.0-py3-none-any.whl (587.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from en-core-web-lg==3.4.0) (3.4.0)\nRequirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (0.9.1)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.0.7)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.4.3)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (4.64.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (59.8.0)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.27.1)\nRequirement already satisfied: thinc<8.2.0,>=8.1.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (8.1.0)\nRequirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (0.6.2)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.21.6)\nRequirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (0.4.1)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.0.7)\nRequirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (4.1.1)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.0.2)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.0.6)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.0.6)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.8.2)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.0.9)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.1.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.3.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.8.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.0.9)\nRequirement already satisfied: smart-open<6.0.0,>=5.2.1 in /opt/conda/lib/python3.7/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (5.2.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2022.6.15)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.0.12)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (1.26.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (3.3)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (0.7.8)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (8.0.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (2.0.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.0) (4.12.0)\nInstalling collected packages: en-core-web-lg\n  Attempting uninstall: en-core-web-lg\n    Found existing installation: en-core-web-lg 2.3.1\n    Uninstalling en-core-web-lg-2.3.1:\n      Successfully uninstalled en-core-web-lg-2.3.1\nSuccessfully installed en-core-web-lg-3.4.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_lg')\n","output_type":"stream"}]},{"cell_type":"code","source":"from scipy import spatial # to compute the distance between words vectors\nimport spacy # to load a word embedding\n\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-07-20T14:17:08.060119Z","iopub.execute_input":"2022-07-20T14:17:08.060586Z","iopub.status.idle":"2022-07-20T14:17:12.519716Z","shell.execute_reply.started":"2022-07-20T14:17:08.060530Z","shell.execute_reply":"2022-07-20T14:17:12.518654Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"nlp = spacy.load('en_core_web_lg')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-07-20T14:17:12.521058Z","iopub.execute_input":"2022-07-20T14:17:12.522500Z","iopub.status.idle":"2022-07-20T14:17:14.827434Z","shell.execute_reply.started":"2022-07-20T14:17:12.522436Z","shell.execute_reply":"2022-07-20T14:17:14.825781Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## The Distribution Hypothesis\n<hr style=\"border:2px solid gray\"> </hr>\n\nWord embeddings are built on top of something called the <em>Distribution Hypothesis</em>.\n\nThe Distribution Hypothesis, in plain English, is the idea that in large bodies of text, certain words will probably appear more closely to eachother than than words that are unrelated.\n\nEmbeddings can be trained using data, such as from Wikipedia, news articles, or questions and answers, and achieve some shocking results. \n\nThe distance between these vectors can be calculated by using cosine similarity.","metadata":{}},{"cell_type":"markdown","source":"Let's say that we have A1, A2, B1 and B2.\nThere is almost the same relationship between A1 and A2, and between B1 and B2 (for instance, between man and woman, and between king and queen).\n\nThen, we have \nA1 - A2 = B1 - B2\n\nSo, an approximation of B2 is equal to\n\nB2' = B1 - (A1 - A2)\n\nB2' = B1 - A1 + A2","metadata":{}},{"cell_type":"code","source":"cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-07-20T14:17:14.830180Z","iopub.execute_input":"2022-07-20T14:17:14.830978Z","iopub.status.idle":"2022-07-20T14:17:15.063667Z","shell.execute_reply.started":"2022-07-20T14:17:14.830925Z","shell.execute_reply":"2022-07-20T14:17:15.062587Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"A1 = nlp.vocab['female'].vector\nA2 = nlp.vocab['male'].vector\n\nB1 = nlp.vocab['queen'].vector\n\n# We now need to find the closest vector in the vocabulary to the result of \"man\" - \"woman\" + \"queen\"\nB2_approximation = B1 - A1 + A2\n#approximation of king = queen - female + male\n\ncomputed_similarities = []\n \nfor word in nlp.vocab:\n    # Ignore words without vectors\n    if not word.has_vector:\n        continue\n \n    similarity = cosine_similarity(B2_approximation, word.vector)\n    computed_similarities.append((word, similarity))\n \ncomputed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\nprint('Closest words in the vector space: ')\nprint([w[0].text for w in computed_similarities[:9]])","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-07-20T14:17:15.065302Z","iopub.execute_input":"2022-07-20T14:17:15.065759Z","iopub.status.idle":"2022-07-20T14:17:15.115136Z","shell.execute_reply.started":"2022-07-20T14:17:15.065711Z","shell.execute_reply":"2022-07-20T14:17:15.113887Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Closest words in the vector space: \n['queen', 'male', 'female', 'she', 'nothin’', 'r.', 'She', 'somethin’', 'Nothin’']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Here, are the most similar words to the vector. It works suprisingly well, and even includes slang. Cuz is short for Cousin, and is an informal way to address a friend, such as calling someone dude or bro.","metadata":{}},{"cell_type":"markdown","source":"### Dimensionality Reduction\n<hr style=\"border:2px solid gray\"> </hr>\n","metadata":{"execution":{"iopub.execute_input":"2022-03-08T09:15:27.187663Z","iopub.status.busy":"2022-03-08T09:15:27.187304Z","iopub.status.idle":"2022-03-08T09:15:27.194075Z","shell.execute_reply":"2022-03-08T09:15:27.192999Z","shell.execute_reply.started":"2022-03-08T09:15:27.187625Z"}}},{"cell_type":"markdown","source":"If we go back to the encoding examples, then it shows us how different representation can expand the feature space tremendously. Embeddings, once trained are suprisingly compared to a one hot dictionary encoding. ","metadata":{}},{"cell_type":"code","source":"A1.shape","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-07-20T14:18:19.650394Z","iopub.execute_input":"2022-07-20T14:18:19.650841Z","iopub.status.idle":"2022-07-20T14:18:19.659939Z","shell.execute_reply.started":"2022-07-20T14:18:19.650808Z","shell.execute_reply":"2022-07-20T14:18:19.659142Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(300,)"},"metadata":{}}]},{"cell_type":"markdown","source":"The embedding for a word, in this model, is embodied in a one dimension vector, of size 300.\nBut we can also consider this as a point in a 300 D space (it would have 300 coordinates). And we can use Principal Component Analysis to represent it in 2 or 3D space.","metadata":{}}]}