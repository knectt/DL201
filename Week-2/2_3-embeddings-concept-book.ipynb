{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Week 2: Embeddings Concept Book","metadata":{}},{"cell_type":"markdown","source":"# Introduction\n<hr style=\"border:2px solid gray\"> </hr>\n","metadata":{}},{"cell_type":"markdown","source":"In the previous notebook, we learned about categorical variables, how they present challenges when putting them into models. With language this could seem insurmountable. We are no longer dealing with dozens or hundreds of unique values in a category. \n\nBut instead, have to work with tens of thousands of words. Not only do these words have meanings by themselves, but instead have meanings relative to each other .However, in the last five years, breakthroughs have occurred that make this possible. \n\nWord embeddings can solve this problem by representing words, not as text, but mathematically as a vector. ","metadata":{}},{"cell_type":"markdown","source":"### Goals and Objectives\n<hr style=\"border:2px solid gray\"> </hr>\n","metadata":{}},{"cell_type":"markdown","source":"* Have an apperciation for how words can be represented as vectors \n* Get a feel for what the distribution hypothesis is\n* See the potential of embeddings through applying word vectors in a short tutorial","metadata":{}},{"cell_type":"markdown","source":"### Key Ideas\n<hr style=\"border:2px solid gray\"> </hr>\n\n* Vector Encoding\n* Feature Space\n* Distribution Hypothesis\n* Dimensionality Reduction","metadata":{}},{"cell_type":"code","source":"!pip install -Uqqq spaCy \n!python -m spacy download en\n!python -m spacy download en_core_web_lg\n\nfrom scipy import spatial\n\nfrom IPython.display import clear_output\nclear_output()\n\n\nimport spacy\nnlp = spacy.load('en_core_web_lg')\n","metadata":{"execution":{"iopub.status.busy":"2022-03-08T09:35:03.595189Z","iopub.execute_input":"2022-03-08T09:35:03.596442Z","iopub.status.idle":"2022-03-08T09:37:17.040234Z","shell.execute_reply.started":"2022-03-08T09:35:03.596393Z","shell.execute_reply":"2022-03-08T09:37:17.039219Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## The Distribution Hypothesis\n<hr style=\"border:2px solid gray\"> </hr>\n\n\nWord embeddings are built on top of something called the Distribution Hypothesis.\n\nThe Distribution Hypothesis, in plain English, is the idea that in large bodies of text, certain words will probably appear more closely to eachother than than words that are unrelated.\n\nEmbeddings can be trained using data, such as from Wikipedia, news articles, or questions and answers, and achieve some shocking results. \n\nThe distance between these vectors can be calculated by using cosine similarity.","metadata":{}},{"cell_type":"code","source":" \ncosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n \nman = nlp.vocab['man'].vector\nwoman = nlp.vocab['woman'].vector\nqueen = nlp.vocab['queen'].vector\nking = nlp.vocab['king'].vector\n \n# We now need to find the closest vector in the vocabulary to the result of \"man\" - \"woman\" + \"queen\"\nmaybe_king = man - woman + queen\ncomputed_similarities = []\n \nfor word in nlp.vocab:\n    # Ignore words without vectors\n    if not word.has_vector:\n        continue\n \n    similarity = cosine_similarity(maybe_king, word.vector)\n    computed_similarities.append((word, similarity))\n \ncomputed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\nprint('Closest words in the vector space to man - woman + queen')\nprint([w[0].text for w in computed_similarities[:9]])","metadata":{"execution":{"iopub.status.busy":"2022-03-08T10:01:12.212144Z","iopub.execute_input":"2022-03-08T10:01:12.212498Z","iopub.status.idle":"2022-03-08T10:01:12.270737Z","shell.execute_reply.started":"2022-03-08T10:01:12.212465Z","shell.execute_reply":"2022-03-08T10:01:12.269641Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"Try your own below by modifying the code ","metadata":{}},{"cell_type":"code","source":"\n\nman = nlp.vocab['man'].vector\nwoman = nlp.vocab['woman'].vector\nqueen = nlp.vocab['queen'].vector\nking = nlp.vocab['king'].vector\n \n# We now need to find the closest vector in the vocabulary to the result of \"man\" - \"woman\" + \"queen\"\nmaybe_king = man - woman + queen\n\n\ncomputed_similarities = []\n \nfor word in nlp.vocab:\n    # Ignore words without vectors\n    if not word.has_vector:\n        continue\n \n    similarity = cosine_similarity(maybe_king, word.vector)\n    computed_similarities.append((word, similarity))\n \ncomputed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\nprint('Closest words in the vector space: ')\nprint([w[0].text for w in computed_similarities[:9]])","metadata":{"execution":{"iopub.status.busy":"2022-03-08T10:05:35.345865Z","iopub.execute_input":"2022-03-08T10:05:35.346235Z","iopub.status.idle":"2022-03-08T10:05:35.355273Z","shell.execute_reply.started":"2022-03-08T10:05:35.346199Z","shell.execute_reply":"2022-03-08T10:05:35.353939Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"Here, are the most similar words to the vector. It works suprisingly well, and even includes slang. Cuz is short for Cousin, and is an informal way to address a friend, such as calling someone dude or bro.","metadata":{}},{"cell_type":"markdown","source":"### Dimensionality Reduction\n<hr style=\"border:2px solid gray\"> </hr>\n","metadata":{"execution":{"iopub.status.busy":"2022-03-08T09:15:27.187304Z","iopub.execute_input":"2022-03-08T09:15:27.187663Z","iopub.status.idle":"2022-03-08T09:15:27.194075Z","shell.execute_reply.started":"2022-03-08T09:15:27.187625Z","shell.execute_reply":"2022-03-08T09:15:27.192999Z"}}},{"cell_type":"markdown","source":"If we go back to the encoding examples, then it shows us how different representation can expand the feature space tremendously. Embeddings, once trained are suprisingly compared to a one hot dictionary encoding. ","metadata":{}},{"cell_type":"code","source":"king.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-08T10:04:38.319917Z","iopub.execute_input":"2022-03-08T10:04:38.320650Z","iopub.status.idle":"2022-03-08T10:04:38.326618Z","shell.execute_reply.started":"2022-03-08T10:04:38.320606Z","shell.execute_reply":"2022-03-08T10:04:38.325760Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"The embedding for a word, in this model, is embodied in only one dimension, which is just 300 numbers long.\n\n* Since it is 1D, if we go back to the broadcasting rules, we learned before, embeddings can be flexibily applied to higher dimensional objects, since we only have to worry about the length rather than multiple dimensions.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
