{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Embeddings Concept Book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<hr style=\"border:2px solid gray\"> </hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, we learned about categorical variables and the challenge to put them into AI models. Here, we work with the human languages, made of dozens of thousands of different tokens. This problem seems insurmountable, because we are no longer dealing with a small amount of categories anymore.\n",
    "\n",
    "Here, as we did with the latent factors in the recommender systems, we represent each word as a large vector. Not only does this solve the problem of encoding, but also this is creating very meaningful coordinates, enable to represent the similarities between words, finding homonyms, synonyms, and much more !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals and Objectives\n",
    "<hr style=\"border:2px solid gray\"> </hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Have an appreciation on the way words can be represented as vectors \n",
    "* Grasp the concept behind the distribution hypothesis\n",
    "* See the potential of embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Ideas\n",
    "<hr style=\"border:2px solid gray\"> </hr>\n",
    "\n",
    "* Vector Encoding\n",
    "* Feature Space\n",
    "* Distribution Hypothesis\n",
    "* Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_kaggle = False   # True if you are on Kaggle, False for local Windows, Linux or Mac environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# libraries installation\n",
    "if is_kaggle:\n",
    "    !pip install -Uqqq spaCy \n",
    "    !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import spatial # to compute the distance between words vectors\n",
    "import spacy # to load a word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Distribution Hypothesis\n",
    "<hr style=\"border:2px solid gray\"> </hr>\n",
    "\n",
    "Word embeddings are built on top of something called the <em>Distribution Hypothesis</em>.\n",
    "\n",
    "The Distribution Hypothesis, in plain English, is the idea that in large bodies of text, certain words will probably appear more closely to eachother than than words that are unrelated.\n",
    "\n",
    "Embeddings can be trained using data, such as from Wikipedia, news articles, or questions and answers, and achieve some shocking results. \n",
    "\n",
    "The distance between these vectors can be calculated by using cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we have A1, A2, B1 and B2.\n",
    "There is almost the same relationship between A1 and A2, and between B1 and B2 (for instance, between man and woman, and between king and queen).\n",
    "\n",
    "Then, we have \n",
    "A1 - A2 = B1 - B2\n",
    "\n",
    "So, an approximation of B2 is equal to\n",
    "\n",
    "B2' = B1 - (A1 - A2)\n",
    "\n",
    "B2' = B1 - A1 + A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest words in the vector space: \n",
      "['queen', 'king', 'boy', 'male']\n"
     ]
    }
   ],
   "source": [
    "A1 = nlp.vocab['female'].vector\n",
    "A2 = nlp.vocab['male'].vector\n",
    "\n",
    "B1 = nlp.vocab['queen'].vector\n",
    "\n",
    "# We now need to find the closest vector in the vocabulary to the result of \"man\" - \"woman\" + \"queen\"\n",
    "B2_approximation = B1 - A1 + A2\n",
    "#approximation of king = queen - female + male\n",
    "\n",
    "computed_similarities = []\n",
    " \n",
    "for word in nlp.vocab:\n",
    "    # Ignore words without vectors\n",
    "    if not word.has_vector:\n",
    "        continue\n",
    " \n",
    "    similarity = cosine_similarity(B2_approximation, word.vector)\n",
    "    computed_similarities.append((word, similarity))\n",
    " \n",
    "computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\n",
    "print('Closest words in the vector space: ')\n",
    "print([w[0].text for w in computed_similarities[:9]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, are the most similar words to the vector. It works suprisingly well, and even includes slang. Cuz is short for Cousin, and is an informal way to address a friend, such as calling someone dude or bro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-08T09:15:27.187663Z",
     "iopub.status.busy": "2022-03-08T09:15:27.187304Z",
     "iopub.status.idle": "2022-03-08T09:15:27.194075Z",
     "shell.execute_reply": "2022-03-08T09:15:27.192999Z",
     "shell.execute_reply.started": "2022-03-08T09:15:27.187625Z"
    }
   },
   "source": [
    "### Dimensionality Reduction\n",
    "<hr style=\"border:2px solid gray\"> </hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we go back to the encoding examples, then it shows us how different representation can expand the feature space tremendously. Embeddings, once trained are suprisingly compared to a one hot dictionary encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "king.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding for a word, in this model, is embodied in a one dimension vector, of size 300.\n",
    "But we can also consider this as a point in a 300 D space (it would have 300 coordinates). And we can use Principal Component Analysis to represent it in 2 or 3D space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
