{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## ðŸ’» UnpackAI DL201 Bootcamp - Week 2 - Skills: Preprocessing Image Data\n\n### ðŸ“• Learning Objectives\n\n* Have an appreciation for the diversity of image preprocessing tasks in various situations and that no one size fits all solution exists\n* Gain an understanding of **spacial filtering**, the principles of how it works from a code based perspective, and how to apply to remove noise, sharpen edges, and detect edges\n* Grasp the concept of thresholding, and appreciate how it can be a powerful tool\n* Understand the difference between local and global thresholding, and the challenges involved. \n\n### ðŸ“– Concepts map\n\n* Spacial Filtering\n* Thresholding\n* Image Masking\n* Global vs Local\n\n\n","metadata":{}},{"cell_type":"code","source":"# import (use not verbose mode : ex \"import -Uqq pandas as pd\" if you are sure that there is no dependency error)\nimport numpy as np\nimport cv2\nfrom matplotlib import pyplot as plt\nimport os\nfrom pathlib import Path\nfrom PIL import Image\nimport IPython.display\nimport wand.image  # requires Imagemagick and !pip install Wand","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import data and images if necessary, and choose the right path\nis_kaggle = True   # True if you are on Kaggle, False for local Windows, Linux or Mac environments.\n\nif is_kaggle:\n    !git clone -b Cohort_7 https://github.com/unpackAI/DL201.git\n    IMAGE_DIR = Path('/kaggle/working/DL201/img')\n    DATA_DIR = Path('/kaggle/working/DL201/data')\nelse:\n    # This section is for local execution, it is assumed that the notebook is on the 'Week-2' folder\n    # of the DL201 repository.\n    DATA_DIR = Path('../data')\n    IMAGE_DIR = Path('../img')\n\n# finally, check if we found the right pathes\nif os.path.isdir(DATA_DIR):\n    print(f'DATA_DIR is a directory, its path is {DATA_DIR}')\nelse:\n    print(\"ERROR : DATA_DIR is not a directory\")\n\nif os.path.isdir(IMAGE_DIR):\n    print(f'IMAGE_DIR is a directory, its path is {IMAGE_DIR}')\nelse:\n    print(\"ERROR : IMAGE_DIR is not a directory\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# picture display function\ndef PictureDisplayJPG(picture_name):\n    image_file_path = os.path.join(IMAGE_DIR, \"week2\" , picture_name)\n    if os.path.isfile(image_file_path):\n        img = cv2.imread(image_file_path)\n        img_array = np.array(img)\n        plt.imshow(img_array)\n        plt.show()\n    \ndef PictureDisplayPNG(picture_name):\n    image_file_path = os.path.join(IMAGE_DIR, \"week2\" , picture_name)\n    if os.path.isfile(image_file_path):\n        img = cv2.cvtColor(cv2.imread(image_file_path), cv2.COLOR_BGR2RGB)\n        img_array = np.array(img)\n        plt.imshow(img_array)\n        plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# control the size of the pictures\nplt.rcParams['figure.figsize'] = [20, 20]\nimage_width = 1000\nimage_height = 1000\n# use 1000 1000 to get bigger pictures","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 1. Introduction","metadata":{}},{"cell_type":"markdown","source":"## 1.1 Why do we need pre-processing\n\nThere are several grades of image pre-processing. Some will only slightly improve the quality of the images (ex : noise reduction) so that the AI model will work better, while others will bring consistent modifications on the original images (ex : background suppression), enlighting shapes or features much more easy to work with than conventional images.\n\nIn some situations, data may not need preprocessing if the model is trained on really various images (ex : uploaded by different users, using different cameras, taking pictures in a variety of lighting conditions, etc) while others, such in an industrial setting, may require fine tuned preprocessing steps to increase the performance of their model (ex : highlight defective products that have to be removed from a production line). ","metadata":{"tags":[]}},{"cell_type":"code","source":" PictureDisplayPNG(\"cat_without_background.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 Do I need to know advanced mathemathics to preprocess images? \n\nWhile there's always some level of math involved in the pre-processing algorithms, python make them easy to call as pre-defined functions. So, most of the time, you just need to know the name of the algorithm and do not need to implement it by your self. In those cases, no math is required.\n\nLikewise, you don't need to impliment a Fourier transform from scratch, as long as you can understand the inputs and outputs of it, and what they mean towards accomplishing the objective of cleaning and enhancing the data.\n\nBefore deep learning, one of the most common ways to extract features was SIFT and its evolutions like SURF. SIFT was very complex algorithms that few people understood completely, but it is heavily cited because everyone used it.","metadata":{}},{"cell_type":"code","source":"PictureDisplayJPG(\"SURF_Homography.jpg\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, most deep-learning methods and their pre-processing steps are mostly based on simple matrix operations, such as convolutional products. Let's notice that this convolutional product is the same as the one used on the convolutional neural networks, which are the base of the most famous pictures classification AI models.\n\nThe following picture was extracted from this website : https://scientistcafe.com/ids/convolutional-neural-network.html","metadata":{}},{"cell_type":"code","source":"PictureDisplayPNG(\"convolutionsbs.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.3 The main methods of image pre-processing","metadata":{}},{"cell_type":"markdown","source":"There are many groups of image-preprocessing. Common ones are :\n\n* Color transforms\n* Filtering\n    - Spacial filters\n    - Frequency filters\n* Thresholding\n    \nIn this notebook, we'll briefly present color transforms, before giving more details on ***Thresholding*** and ***Spacial Filters*** because they are related to the concepts learned in the first week and build on top of them.","metadata":{}},{"cell_type":"markdown","source":"# Part 2. Color Transforms\n\nColor transforms consist in changing the dimension of an image to make it easier to work with, by human being or by an AI model.\n\nThe two most common examples is the transform between RGB (red green blue) and grayscale images.\n$$ RGB --> grayscale $$\n\nBut if you work with images made for printers, then you may have to work with CMYK (cyan, magenta, yellow, black) images, that you would have to convert into RGB ones.\n$$ CMYK --> RGB $$\n\nFinally, if you use a pretrained model that was trained on color images, you should consider transforming your grayscale images into color images.\n$$ grayscale --> RGB $$\n\nIt may make sense to preprocess X-rays and MRIs using this technique, because X-rays and MRIs are often black and white or monochrome. By expanding the color spectrum to include more color, it can highlight features that would be more difficult to spot otherwise.","metadata":{}},{"cell_type":"code","source":"braintumor = Image.open(IMAGE_DIR/'week2/braintumor.jpg')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grayscale = np.array(braintumor)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IPython.display.Image(IMAGE_DIR/'week2/braintumor.jpg', width=image_width, height=image_height)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pseudoColoredImage = cv2.applyColorMap(grayscale, cv2.COLORMAP_JET)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(pseudoColoredImage)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'shape of the grayscale image : {grayscale.shape}')\nprint(f'shape of the pseudo color image : {pseudoColoredImage.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 3. Spatial Filters","metadata":{}},{"cell_type":"markdown","source":"## 3.1 Definition\nA Spacial Filter uses the principle of using the information in neighboring pixels to be able to correct, restore, or enhance the image. ","metadata":{}},{"cell_type":"markdown","source":"## 3.2 How they work\nThis is done using a mask, which is a small 3x3 or larger matrix that is moved across all of the pixels. Here is an example where we will add some noise to an image.","metadata":{}},{"cell_type":"code","source":"example_img_path = str(IMAGE_DIR/'week2/chimpanzee.jpg')\nnoisy_img_path = str(IMAGE_DIR/'week2'/\"noisyExample.jpg\")\nnoisy_img_path2 = str(IMAGE_DIR/'week2'/\"noisyExample2.jpg\")\n\nexample_img = cv2.imread(example_img_path)\n# Read image using Image() function\nwith wand.image.Image(filename=example_img_path)as noisy_img:  \n    noisy_img.noise(\"gaussian\", attenuate = 2)\n    noisy_img.save(filename = noisy_img_path)\n    \nnoisy_img1 = cv2.imread(noisy_img_path)\n    \nwith wand.image.Image(filename=example_img_path)as noisy_img:  \n    noisy_img.noise(\"poisson\", 0.5, \"green\")\n    noisy_img.save(filename = noisy_img_path2)\n    \nnoisy_img2 = cv2.imread(noisy_img_path2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IPython.display.Image(example_img_path, width=image_width, height=image_height)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IPython.display.Image(noisy_img_path, width=image_width, height=image_height)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IPython.display.Image(noisy_img_path2, width=image_width, height=image_height)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 Example of space filter : Noise Removal (Blurring)","metadata":{}},{"cell_type":"markdown","source":"A mean filter takes the values of all the pixels surrounding it, and applying it to that pixel. It does this across the entire image.","metadata":{}},{"cell_type":"code","source":"means_filtered_img = cv2.fastNlMeansDenoisingColored(noisy_img1,None,10,10,7,21)\ngaussian_blurred_img = cv2.GaussianBlur(noisy_img1, (5, 5), 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(cv2.cvtColor(means_filtered_img, cv2.COLOR_BGR2RGB))\n# as opencv loads in BGR format by default, we want to show it in RGB.\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(cv2.cvtColor(gaussian_blurred_img, cv2.COLOR_BGR2RGB))\n# as opencv loads in BGR format by default, we want to show it in RGB.\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Homeworks*** : search for the signification of the parameters of fastNlMeansDenoisingColored, try to modify them to get the better results on noisy_img1, and then, apply the same noise removal process on noisy_img2.","metadata":{}},{"cell_type":"markdown","source":"## 3.4 Example of space filter : Image Enhancement","metadata":{}},{"cell_type":"markdown","source":"After image correction, Spacial Filters can be used to ***sharpen images***.","metadata":{}},{"cell_type":"code","source":"kernel = np.array([[0,-2,0],\n                   [-2,9,-2],\n                   [0,-2,0]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The 9 in the middle of this kernel will enlight the edges and borders, while the zero and negative values will let the \"flat\" areas unchanged.","metadata":{}},{"cell_type":"code","source":"grayscale_example = cv2.cvtColor(example_img, cv2.COLOR_RGB2GRAY)\nimg_sharp = cv2.filter2D(grayscale_example,-1, kernel)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(grayscale_example, cmap='gray')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(img_sharp, cmap='gray')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.5 Edge detection","metadata":{}},{"cell_type":"markdown","source":"As you may have learnt or will learn about convolutional neural networks, spacial kernels can be used to \"detect\" different shapes in the image.\nBelow, you can see that it is possible to find the horizontal and vertical 'edges' of an image.","metadata":{}},{"cell_type":"code","source":"image_file_path = str(IMAGE_DIR/'week2'/\"vertical_edges_horizontal_edges.png\")\nimg = cv2.cvtColor(cv2.imread(image_file_path), cv2.COLOR_BGR2RGB)\nimg_array = np.array(img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_array.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display the first third\nplt.imshow(img_array[:,0:1878//3,:])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display the second third\nplt.imshow(img_array[:,1878//3:2*1878//3,:])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display the third third\nplt.imshow(img_array[:,2*1878//3:,:])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 4. Image segmentation and thresholding ","metadata":{"tags":[]}},{"cell_type":"markdown","source":"## 4.1 Definitions\nImage segmentation is a group of techniques in digital image processing aiming at analysing and partition an image into multiple parts or regions.\n\nIn digital image processing, thresholding is the simplest method of segmenting images. On gray scale images, thresholding is the fact of choosing a pixel value T, and dividing the image in 2 parts :\n* the part 1 including the pixels the gray value of which is lower than T\n* the part 2 including the pixels the gray value of which is superior or greater than T\n\nThresholding can be used to separate the foreground and background of an image.","metadata":{}},{"cell_type":"markdown","source":"Here, we are using thresholding to create a binary images. The ensemble of 0 and 1 can be used as ***indexes*** for our foreground and background.","metadata":{}},{"cell_type":"code","source":"mask = example_img.copy()\nvalue_threshold = 90\nbackground = (grayscale_example <= value_threshold)\nforeground = (grayscale_example > value_threshold)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = Image.fromarray(background)\nplt.imshow(image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = Image.fromarray(foreground)\nplt.imshow(image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2 When is thresholding used?","metadata":{}},{"cell_type":"markdown","source":"The main purpose of thresholding is to grasp one part of the image (ex the darker part) and then apply some transformations only on this part (ex : lighting).\n\nOnce we have identified a threshold, it becomes possible to preprocess an image in the areas where it needs it most.\n\nThis solves a variety of problems such as uneven lighting, hidden features, or even noise present in the background which may affect the model's performance.\n\nIdentifying a threshold is an important part of image preprocessing because it allows one to be much more precise.","metadata":{}},{"cell_type":"markdown","source":"## 4.3 How can a threshold be identified?","metadata":{}},{"cell_type":"markdown","source":"There are many ways to identify a threshold, but the most simple one is by taking the mean.\n\nIf we have access to the histogram of the colors, then we are looking for two normal distributions, and will choose a threshold separating them clearly.","metadata":{}},{"cell_type":"code","source":"# tuple to select colors of each channel line\ncolors = (\"red\", \"green\", \"blue\")\nchannel_ids = (0, 1, 2)\n\n# create the histogram plot, with three lines, one for\n# each color\nplt.figure()\nplt.xlim([0, 256])\nfor channel_id, c in zip(channel_ids, colors):\n    histogram, bin_edges = np.histogram(\n        example_img[:, :, channel_id], bins=256, range=(0, 256)\n    )\n    plt.plot(bin_edges[0:-1], histogram, color=c)\n\nplt.title(\"Color Histogram\")\nplt.xlabel(\"Color value\")\nplt.ylabel(\"Pixel count\")\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This graph shows two distinct places where many values are contained and identifies where to put a threshold. ","metadata":{}},{"cell_type":"markdown","source":"## 4.4 Draw the edges based on spacial filtering and thresholding","metadata":{}},{"cell_type":"markdown","source":"Here, we use a spacial filter to find the ***edges***, and then, we use two thresholds to link the edges together and find the initial segments of strong edges.","metadata":{}},{"cell_type":"code","source":"# Thresholds \n#t_lower = 150\n#t_upper = 210\nt_lower = 100\nt_upper = 140\n\n#edge = cv2.Canny(gaussian_blurred_img, t_lower, t_upper)\nedge = cv2.Canny(grayscale_example, t_lower, t_upper)\n\nplt.imshow(edge, cmap='gray')\n# as opencv loads in BGR format by default, we want to show it in RGB.\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Homeworks*** : Try several combinations of t_upper and t_lower to get the best edge picture. Then, try this combination on another picture (ex : gaussian_blurred_img) and see if you can keep those parameters to get good results.","metadata":{}},{"cell_type":"markdown","source":"## 4.5 Background Removal","metadata":{}},{"cell_type":"code","source":"foreground = np.where(foreground[...,None],example_img,0)\nbackground = np.where(background[...,None],example_img,0)\nplt.imshow(cv2.cvtColor(foreground, cv2.COLOR_BGR2RGB))\n# as opencv loads in BGR format by default, we want to show it in RGB.\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_added = foreground + background\nplt.imshow(cv2.cvtColor(img_added, cv2.COLOR_BGR2RGB))\n# as opencv loads in BGR format by default, we want to show it in RGB.\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.6 Brightness adjustment","metadata":{}},{"cell_type":"markdown","source":"Here, the pixel values are modified linearly. If it were grayscale image, we would have :\n$$ P2 = Alpha . P1 + Beta$$\n* alpha > 1 increases the contrast\n* alpha < 1 decreases the contrast\n* beta > 1 makes the picture lighter\n* beta < 1 makes the picture darker","metadata":{}},{"cell_type":"code","source":"alpha = 1.5\nbeta = 50\nbackground_adjusted = cv2.convertScaleAbs(background, alpha=alpha, beta=beta)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_adjusted = foreground + background_adjusted\nplt.imshow(cv2.cvtColor(img_adjusted, cv2.COLOR_BGR2RGB))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Thanks to this adjustment, the foreground is not too light, and we have much more details on the chimpanzees.","metadata":{}},{"cell_type":"markdown","source":"***Homeworks*** : Why could you propose to get a better result ? As soon as you get an answer, try it by improving or completing the code below.","metadata":{}}]}