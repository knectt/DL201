{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unpackAI/DL201/blob/Cohort_7/Week-2/2_6_NLP_Preprocessing_Book.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94a244d1",
      "metadata": {
        "id": "94a244d1"
      },
      "source": [
        "## ðŸ’» UnpackAI DL201 Bootcamp - Week 2 - Skills: NLP\n",
        "\n",
        "### ðŸ“• Learning Objectives\n",
        "\n",
        "* Reinforce the need for data processing not only for NLP but for most machine learning tasks.\n",
        "* Review common data processing steps for NLP tasks.\n",
        "\n",
        "### ðŸ“– Concepts map\n",
        "* Text fomratting\n",
        "* Tokenization\n",
        "* Stemming\n",
        "* Lemmatization\n",
        "* Stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e75d726",
      "metadata": {
        "id": "9e75d726"
      },
      "source": [
        "As in most machine learning tasks, data preprocessing is a key step in the process of training a model as faulty and poor quality data will result in poor performance. Text preprocessing in NLP represent the set of techniques that format and correct the structure of the text, remove unwanted characters and words, simplify and highlight the semantical meaninig of the text as well as transforms the text into a form that can be used by the machine learning algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d37cf43",
      "metadata": {
        "id": "3d37cf43"
      },
      "source": [
        "Preprocessing tasks are more standarized than the ones use for Computer Vision and tabular data analysis, altough there are differences (because not all tasks require the same level of preprocessing), some steps are reused, often in the same order. Below there is a brief description of some of these tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56b6775c",
      "metadata": {
        "id": "56b6775c"
      },
      "source": [
        "These tasks are part of the **morphological and lexical analysis** of the text, which are at the bottom of the NLP pipeline (text matching)\n",
        "\n",
        "- Text Integration: Combining text from different sources into a single corpus. (sample image from: https://www.opinosis-analytics.com)\n",
        "\n",
        "<img src=https://i1.wp.com/www.opinosis-analytics.com/wp-content/uploads/2020/01/customer-support-enquiry-sources.png alt=\"alt text\" title=\"image Title\" height=\"300\"/>\n",
        "\n",
        "- Text Formatting: Cleaning and formatting text.\n",
        "    - Removal of punctuation.\n",
        "    - Lowercasing.\n",
        "    - Removal of stopwords. (sample image source: https://lionbridge.ai/wp-content/uploads/2019/10/lm_02.png)\n",
        "    - Removal of numbers (or replace them with word numbers)\n",
        "    - Removal of special characters (e.g. HTML tags, URLs, string patterns, etc.)\n",
        "    - Removal of short words (e.g. words with less than 3 characters)\n",
        "    - Removal of repeated words.\n",
        "    - Removal of rare words (e.g. words that appear only once or only in a few documents).\n",
        "\n",
        "<img src=https://lionbridge.ai/wp-content/uploads/2019/10/lm_02.png alt=\"alt text\" title=\"image Title\" height=\"300\"/>\n",
        "\n",
        "- Text segementation: Splitting text into sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9060def4",
      "metadata": {
        "id": "9060def4"
      },
      "source": [
        "Other processing steps operate at the **sematic level**, which affects the meaning of the text. (often rule-based)\n",
        "\n",
        "- Spell checking: Correcting misspelled words.\n",
        "- Grammar checking: Correcting grammatical errors.\n",
        "- Stemming: Removing suffixes from words.\n",
        "- Lemmatization: Simplifying words by using a dictionary of known words and roots."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a00f6e33",
      "metadata": {
        "id": "a00f6e33"
      },
      "source": [
        "Example of Stemming. From: https://i0.wp.com/trevorfox.com/wp-content/uploads/2018/07/stemming-example.png"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77ee5c66",
      "metadata": {
        "id": "77ee5c66"
      },
      "source": [
        "![](https://i0.wp.com/trevorfox.com/wp-content/uploads/2018/07/stemming-example.png?fit=500%2C605&ssl=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d949456e",
      "metadata": {
        "id": "d949456e"
      },
      "source": [
        "Comparison with lemmatization. From: https://medium.com/swlh/introduction-to-stemming-vs-lemmatization-nlp-8c69eb43ecfe"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "999a8d4e",
      "metadata": {
        "id": "999a8d4e"
      },
      "source": [
        "![](https://tse3-mm.cn.bing.net/th/id/OIP-C.2K4VxxRtewNw4iP-Kh5Z7QHaEH?pid=ImgDet&rs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26b440ba",
      "metadata": {
        "id": "26b440ba"
      },
      "source": [
        "Additionally, text enrichment can be applied providing more semantics to the original text with data that we didn't have before. (machine-learning, learn-based)\n",
        "- POS Tag: Part of speech tagging.\n",
        "- Entity Recognition: Recognizing named entities.\n",
        "- Entity relation extraction: Extracting relations between named entities.\n",
        "- Dependency parsing: Parsing the sentence into a tree structure."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e6244d8",
      "metadata": {
        "id": "7e6244d8"
      },
      "source": [
        "POS Tagging. From: https://www.researchgate.net/publication/337460636_Unpacking_the_Smart_Mobility_Concept_in_the_Dutch_Context_Based_on_a_Text_Mining_Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef9768b9",
      "metadata": {
        "id": "ef9768b9"
      },
      "source": [
        "![](https://www.researchgate.net/publication/337460636/figure/download/fig1/AS:828223747284992@1574475337385/Example-of-part-of-speech-POS-tagging-and-lemmatization-for-two-example-sentences-The.ppm)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a323fdfb",
      "metadata": {
        "id": "a323fdfb"
      },
      "source": [
        "Entity recognition and dependency parsing. From: https://stanfordnlp.github.io/CoreNLP/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd36a084",
      "metadata": {
        "id": "cd36a084"
      },
      "source": [
        "![](https://stanfordnlp.github.io/CoreNLP/assets/images/ner.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7674507",
      "metadata": {
        "id": "f7674507"
      },
      "source": [
        "Entity relation-extraction. From: https://www.mdpi.com/2079-9292/9/10/1637"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQX_pvo9o5Gq"
      },
      "source": [
        "![](https://www.mdpi.com/electronics/electronics-09-01637/article_deploy/html/images/electronics-09-01637-g001.png)"
      ],
      "id": "sQX_pvo9o5Gq"
    },
    {
      "cell_type": "markdown",
      "id": "bd364f68",
      "metadata": {
        "id": "bd364f68"
      },
      "source": [
        "Then we have text vectorization, which is the process of converting the text into a vector representation. This step is required to train a machine learning model.\n",
        "\n",
        "Vectorized representations of text are usually obtained via:\n",
        "- Bag of words: A vector representation of the text is obtained by counting the number of times each word appears in the text.\n",
        "- TF-IDF: (Term Frequency/ Inverse Document Frequency) A vector representation of the text is obtained by counting the number of times each word appears in the text and then normalizing the counts by the number of documents in which the word appears.</br>\n",
        "(Visual example of these 2 techniques can be found at: https://dataaspirant.com/word-embedding-techniques-nlp/)</br>  \n",
        "- Word embeddings: A vector representation of the text is obtained by using a word embedding model to represent the text.\n",
        "    - Word2Vec.\n",
        "    - Bert.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb3aa398",
      "metadata": {
        "id": "bb3aa398"
      },
      "source": [
        "From here the vectorized representation becomes the input for the machine learning algorithm. Depending of the algorithm, the input can be:\n",
        "- Classification: The input is a vector representation of the text and the output is a class label.\n",
        "- Regression: The input is a vector representation of the text and the output is a real number.\n",
        "- Clustering: The input is a vector representation of the text and the output is a cluster label.\n",
        "- Recommendation: The input is a vector representation of the text and the output is a list of recommendations.\n",
        "- Sentiment analysis: The input is a vector representation of the text and the output is a real number.\n",
        "- Topic modeling: The input is a vector representation of the text and the output is a list of topics.\n",
        "- Text summarization: The input is a vector representation of the text and the output is a list of sentences.\n",
        "- Text translation: The input is a vector representation of the text and the output is a list of translations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f71e40e",
      "metadata": {
        "id": "6f71e40e"
      },
      "source": [
        "### Revisit the previous example\n",
        "\n",
        "Implement a few of the preprocessing steps mentioned above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d7237d2f",
      "metadata": {
        "id": "d7237d2f"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import requests\n",
        "import nltk\n",
        "from nltk import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -Uqq transformers"
      ],
      "metadata": {
        "id": "ff3LZ_-apfUj"
      },
      "id": "ff3LZ_-apfUj",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel"
      ],
      "metadata": {
        "id": "Z6JOPum6pap0"
      },
      "id": "Z6JOPum6pap0",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7d58e3a1",
      "metadata": {
        "id": "7d58e3a1",
        "outputId": "3e552e41-ec86-4a29-d969-49a201442cc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# Download dependencies\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "G5i35CG5uMwc",
        "outputId": "0e948a07-07bd-4fcc-b4fc-67ee90cac2d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "G5i35CG5uMwc",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "5340ddd5",
      "metadata": {
        "id": "5340ddd5"
      },
      "outputs": [],
      "source": [
        "# Load a sample text, from the provided url into a dataframe\n",
        "response = requests.get('https://www.gutenberg.org/ebooks/8655.txt.utf-8')\n",
        "sample_text = response.text\n",
        "sentences = sample_text.split('\\n')                        # Split text into sentences\n",
        "df = pd.DataFrame(sentences, columns=['sentence'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "b47c030b",
      "metadata": {
        "id": "b47c030b",
        "outputId": "e77ec1b2-64a3-42b5-850b-9a1445a1d383",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  after removing the cwd from sys.path.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    the project gutenberg ebook of the book of the...\n",
              "1                        night volume i by anonymous\\r\n",
              "3    this ebook is for the use of anyone anywhere i...\n",
              "4    other parts of the world at no cost and with a...\n",
              "5    whatsoever  you may copy it give it away or re...\n",
              "Name: sentence, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Text cleaning (morphological changes)\n",
        "df['sentence'] = df['sentence'].str.lower()                 # Lowercase\n",
        "df = df[df['sentence'].str.split().str.len() > 3]           # Remove short sentences\n",
        "df['sentence'] = df['sentence'].str.replace('[^\\w\\s]','')   # Remove punctuation\n",
        "max_len = df['sentence'].str.len().max()                    # longest sentence\n",
        "df['sentence'].head(5)                                    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "fa22224a",
      "metadata": {
        "id": "fa22224a",
        "outputId": "b25b0ee7-ad50-4e9a-cf9a-3821b38038fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    project gutenberg ebook book thousand nights one\n",
              "1                              night volume anonymous\n",
              "3             ebook use anyone anywhere united states\n",
              "4                parts world cost almost restrictions\n",
              "5           whatsoever may copy give away reuse terms\n",
              "Name: sentence, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Remove Stopwords\n",
        "eng_stopwords = stopwords.words('english')\n",
        "print(eng_stopwords[-10:])\n",
        "df['sentence'] = df['sentence'].apply(lambda x: ' '.join([word for word in x.split() if word not in eng_stopwords]))\n",
        "df['sentence'].head(5) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "39cdd321",
      "metadata": {
        "id": "39cdd321",
        "outputId": "7a30e6da-8b6c-4dd1-8673-44b64fd8fc90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    project gutenberg ebook book thousand night one\n",
              "1                             night volume anonymous\n",
              "3             ebook use anyone anywhere united state\n",
              "4                 part world cost almost restriction\n",
              "5           whatsoever may copy give away reuse term\n",
              "Name: sentence, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# Apply lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['sentence'] = df['sentence'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
        "df['sentence'].head(5) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "24fe779c",
      "metadata": {
        "id": "24fe779c",
        "outputId": "5940821d-1dcf-49c5-bbd9-e69ff346e6c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    project gutenberg ebook book thousand night one\n",
              "1                                 night volum anonym\n",
              "3                 ebook use anyon anywher unit state\n",
              "4                    part world cost almost restrict\n",
              "5              whatsoev may copi give away reus term\n",
              "Name: sentence, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# Apply stemming\n",
        "stemmer = PorterStemmer()\n",
        "df['sentence'] = df['sentence'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
        "df['sentence'].head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "205585c8",
      "metadata": {
        "id": "205585c8",
        "outputId": "3714b10e-b277-4fdb-e398-4c28a5250c04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          sentence  \\\n",
              "1365                  thee one destroy thee answer   \n",
              "2230                   vilest white heard saw pass   \n",
              "10746               befel one day king seat throne   \n",
              "238            idiom otherwis retain strict letter   \n",
              "10639                   way christian broker stori   \n",
              "4847       hundredth door bade farewel depart leav   \n",
              "10261  lord thi wife thi handmaid stand thee deign   \n",
              "5340                  move piti said hear obey god   \n",
              "3369               news wept till swoon away heart   \n",
              "12139                        ill befallen make end   \n",
              "\n",
              "                                      tokenized_sentence  \\\n",
              "1365                  [thee, one, destroy, thee, answer]   \n",
              "2230               [vile, ##st, white, heard, saw, pass]   \n",
              "10746      [be, ##fe, ##l, one, day, king, seat, throne]   \n",
              "238    [id, ##iom, other, ##wi, ##s, retain, strict, ...   \n",
              "10639                [way, christian, broker, st, ##ori]   \n",
              "4847   [hundred, ##th, door, bad, ##e, fare, ##we, ##...   \n",
              "10261  [lord, th, ##i, wife, th, ##i, hand, ##maid, s...   \n",
              "5340             [move, pit, ##i, said, hear, obey, god]   \n",
              "3369          [news, wept, till, sw, ##oon, away, heart]   \n",
              "12139                 [ill, be, ##fall, ##en, make, end]   \n",
              "\n",
              "                                  numericalized_sentence  \n",
              "1365                    [14992, 2028, 6033, 14992, 3437]  \n",
              "2230               [25047, 3367, 2317, 2657, 2387, 3413]  \n",
              "10746   [2022, 7959, 2140, 2028, 2154, 2332, 2835, 6106]  \n",
              "238    [8909, 18994, 2060, 9148, 2015, 9279, 9384, 3661]  \n",
              "10639                   [2126, 3017, 20138, 2358, 10050]  \n",
              "4847   [3634, 2705, 2341, 2919, 2063, 13258, 8545, 21...  \n",
              "10261  [2935, 16215, 2072, 2564, 16215, 2072, 2192, 2...  \n",
              "5340         [2693, 6770, 2072, 2056, 2963, 15470, 2643]  \n",
              "3369        [2739, 24966, 6229, 25430, 7828, 2185, 2540]  \n",
              "12139              [5665, 2022, 13976, 2368, 2191, 2203]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-318861cd-ef27-482c-97f9-0cea7f257f64\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>tokenized_sentence</th>\n",
              "      <th>numericalized_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1365</th>\n",
              "      <td>thee one destroy thee answer</td>\n",
              "      <td>[thee, one, destroy, thee, answer]</td>\n",
              "      <td>[14992, 2028, 6033, 14992, 3437]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2230</th>\n",
              "      <td>vilest white heard saw pass</td>\n",
              "      <td>[vile, ##st, white, heard, saw, pass]</td>\n",
              "      <td>[25047, 3367, 2317, 2657, 2387, 3413]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10746</th>\n",
              "      <td>befel one day king seat throne</td>\n",
              "      <td>[be, ##fe, ##l, one, day, king, seat, throne]</td>\n",
              "      <td>[2022, 7959, 2140, 2028, 2154, 2332, 2835, 6106]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>238</th>\n",
              "      <td>idiom otherwis retain strict letter</td>\n",
              "      <td>[id, ##iom, other, ##wi, ##s, retain, strict, ...</td>\n",
              "      <td>[8909, 18994, 2060, 9148, 2015, 9279, 9384, 3661]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10639</th>\n",
              "      <td>way christian broker stori</td>\n",
              "      <td>[way, christian, broker, st, ##ori]</td>\n",
              "      <td>[2126, 3017, 20138, 2358, 10050]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4847</th>\n",
              "      <td>hundredth door bade farewel depart leav</td>\n",
              "      <td>[hundred, ##th, door, bad, ##e, fare, ##we, ##...</td>\n",
              "      <td>[3634, 2705, 2341, 2919, 2063, 13258, 8545, 21...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10261</th>\n",
              "      <td>lord thi wife thi handmaid stand thee deign</td>\n",
              "      <td>[lord, th, ##i, wife, th, ##i, hand, ##maid, s...</td>\n",
              "      <td>[2935, 16215, 2072, 2564, 16215, 2072, 2192, 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5340</th>\n",
              "      <td>move piti said hear obey god</td>\n",
              "      <td>[move, pit, ##i, said, hear, obey, god]</td>\n",
              "      <td>[2693, 6770, 2072, 2056, 2963, 15470, 2643]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3369</th>\n",
              "      <td>news wept till swoon away heart</td>\n",
              "      <td>[news, wept, till, sw, ##oon, away, heart]</td>\n",
              "      <td>[2739, 24966, 6229, 25430, 7828, 2185, 2540]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12139</th>\n",
              "      <td>ill befallen make end</td>\n",
              "      <td>[ill, be, ##fall, ##en, make, end]</td>\n",
              "      <td>[5665, 2022, 13976, 2368, 2191, 2203]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-318861cd-ef27-482c-97f9-0cea7f257f64')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-318861cd-ef27-482c-97f9-0cea7f257f64 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-318861cd-ef27-482c-97f9-0cea7f257f64');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# Tokenize the sentences, add tokens ids\n",
        "tokens_df = df.copy()\n",
        "tokens_df['tokenized_sentence'] = tokens_df['sentence'].apply(bert_tokenizer.tokenize)\n",
        "tokens_df['numericalized_sentence'] = tokens_df['tokenized_sentence'].apply(bert_tokenizer.convert_tokens_to_ids)\n",
        "tokens_df.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "82dd7f30",
      "metadata": {
        "id": "82dd7f30",
        "outputId": "21d83052-08bf-4f1c-c0f9-85bfc1d0bffa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10211    [101, 15876, 4859, 2099, 16101, 29122, 2165, 1...\n",
              "877      [101, 2034, 2214, 2158, 2358, 10050, 102, 0, 0...\n",
              "5287     [101, 2158, 19549, 2360, 5520, 2048, 102, 0, 0...\n",
              "850      [101, 2214, 2158, 4012, 26569, 2556, 2019, 145...\n",
              "4517     [101, 2067, 22864, 8202, 2604, 2546, 2078, 236...\n",
              "9129     [101, 9765, 4017, 2269, 9099, 20051, 21442, 68...\n",
              "13919    [101, 1057, 4983, 9385, 2025, 2594, 4297, 7630...\n",
              "11351    [101, 2052, 2507, 14992, 2051, 2202, 3272, 102...\n",
              "8834     [101, 2237, 2191, 2705, 2540, 3239, 2095, 2078...\n",
              "11942    [101, 14412, 6305, 4848, 2548, 15547, 3672, 30...\n",
              "Name: numericalized_sentence, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# Add the [CLS] and [SEP] special tokens and padding to the numericalized sentences on the dataframe\n",
        "tokens_df['numericalized_sentence'] = tokens_df['numericalized_sentence'].apply(lambda x: [bert_tokenizer.cls_token_id] + x + [bert_tokenizer.sep_token_id])\n",
        "tokens_df['numericalized_sentence'] = tokens_df['numericalized_sentence'].apply(lambda x: x + [bert_tokenizer.pad_token_id] * (max_len - len(x)))\n",
        "tokens_df['numericalized_sentence'].sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "2344b630",
      "metadata": {
        "id": "2344b630",
        "outputId": "62f46bbd-63ad-4bb0-bb57-cc256d2360a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([12690, 79])\n"
          ]
        }
      ],
      "source": [
        "# Extract encoded value to a Tensor\n",
        "numericalized_sentences = tokens_df['numericalized_sentence'].values\n",
        "numericalized_sentences = [list(x) for x in numericalized_sentences]\n",
        "numericalized_sentences = np.array(numericalized_sentences)\n",
        "numericalized_sentences = torch.from_numpy(numericalized_sentences)\n",
        "print(numericalized_sentences.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "7ed10e8b",
      "metadata": {
        "id": "7ed10e8b",
        "outputId": "cf744c72-494f-401b-b619-53d046b89462",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3172, 79])\n"
          ]
        }
      ],
      "source": [
        "# Use a fourth of the sentences to reduce memory usage\n",
        "numericalized_sentences = numericalized_sentences[:len(numericalized_sentences)//4, :]\n",
        "print(numericalized_sentences.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cfdd7d4",
      "metadata": {
        "id": "3cfdd7d4"
      },
      "outputs": [],
      "source": [
        "# Encode the numericalized sentences using BERT\n",
        "encoded_sentences = bert_model(numericalized_sentences)[0]\n",
        "encoded_sentences = encoded_sentences.detach().numpy()\n",
        "print(encoded_sentences.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4384158",
      "metadata": {
        "id": "f4384158"
      },
      "outputs": [],
      "source": [
        "# Add embedings of each sentence\n",
        "encoded_sentences = np.sum(encoded_sentences, axis=1)\n",
        "print(encoded_sentences.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a9ac81f",
      "metadata": {
        "id": "0a9ac81f"
      },
      "outputs": [],
      "source": [
        "# Use PCA to reduce the embedding dimensionality to 3\n",
        "pca = PCA(n_components=3)\n",
        "pca.fit(encoded_sentences)\n",
        "reduced_embeddings = pca.transform(encoded_sentences)\n",
        "print(reduced_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d77be743",
      "metadata": {
        "id": "d77be743"
      },
      "outputs": [],
      "source": [
        "# plot 3D embeddings, add tight axis\n",
        "fig = plt.figure()\n",
        "ax = Axes3D(fig)\n",
        "ax.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], reduced_embeddings[:, 2])\n",
        "ax.grid(True)\n",
        "\n",
        "\n",
        "# Add a label to each data point\n",
        "for i in range(len(reduced_embeddings)):\n",
        "    ax.text(reduced_embeddings[i, 0], reduced_embeddings[i, 1], reduced_embeddings[i, 2], f'{i}')\n",
        "    \n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d93e4e4a",
      "metadata": {
        "id": "d93e4e4a"
      },
      "outputs": [],
      "source": [
        "# Print sentences that appear distinct in the embeddings\n",
        "print(f\"- Sentence 76: {df['sentence'].iloc[76]}\")\n",
        "print(f\"- Sentence 220: {df['sentence'].iloc[220]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d52596f",
      "metadata": {
        "tags": [],
        "id": "1d52596f"
      },
      "source": [
        "### Exercise: experiment!\n",
        "\n",
        "* Combine text from at least two different sources.\n",
        "* Try different nlp libraries\n",
        "* Perform an expanded NLP pipeline (check spelling, POS tagging, entity recognition, dependency parsing, etc.)\n"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "d316707d93be09212242bb4431563349bc87a6a4965cbb94d95cad94a149faa0"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('DL201')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "2_6_NLP_Preprocessing_Book.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}