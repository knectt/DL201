{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94a244d1",
   "metadata": {},
   "source": [
    "## ðŸ’» UnpackAI DL201 Bootcamp - Week 2 - Skills: NLP\n",
    "\n",
    "### ðŸ“• Learning Objectives\n",
    "\n",
    "* Reinforce the need for data processing not only for NLP but for most machine learning tasks.\n",
    "* Review common data processing steps for NLP tasks.\n",
    "\n",
    "### ðŸ“– Concepts map\n",
    "* Text fomratting\n",
    "* Tokenization\n",
    "* Stemming\n",
    "* Lemmatization\n",
    "* Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e75d726",
   "metadata": {},
   "source": [
    "As in most machine learning tasks, data preprocessing is a key step in the process of training a model as faulty and poor quality data will result in poor performance. Text preprocessing in NLP represent the set of techniques that format and correct the structure of the text, remove unwanted characters and words, simplify and highlight the semantical meaninig of the text as well as transforms the text into a form that can be used by the machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d37cf43",
   "metadata": {},
   "source": [
    "Preprocessing tasks are more standarized than the ones use for Computer Vision and tabular data analysis, altough there are differences (because not all tasks require the same level of preprocessing),some steps are reused, often in the same order. Below there is a brief description of some of these tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b6775c",
   "metadata": {},
   "source": [
    "These tasks are part of the **morphological and lexical analysis** of the text, which are at the bottom of the NLP pipeline (text matching)\n",
    "\n",
    "- Text Integration: Combining text from different sources into a single corpus.\n",
    "- Text Formatting: Cleaning and formatting text.\n",
    "    - Removal of punctuation.\n",
    "    - Lowercasing.\n",
    "    - Removal of stopwords.\n",
    "    - Removal of numbers (or replace them with word numbers)\n",
    "    - Removal of special characters (e.g. HTML tags, URLs, string patterns, etc.)\n",
    "    - Removal of short words (e.g. words with less than 3 characters)\n",
    "    - Removal of repeated words.\n",
    "    - Removal of rare words (e.g. words that appear only once or only in a few documents).\n",
    "- Text segementation: Splitting text into sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9060def4",
   "metadata": {},
   "source": [
    "Other processing steps operate at the **sematic level**, which affects the meaning of the text. (often rule-based)\n",
    "\n",
    "- Spell checking: Correcting misspelled words.\n",
    "- Grammar checking: Correcting grammatical errors.\n",
    "- Stemming: Removing suffixes from words.\n",
    "- Lemmatization: Simplifying words by using a dictionary of known words and roots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00f6e33",
   "metadata": {},
   "source": [
    "Example of Stemming. From: https://i0.wp.com/trevorfox.com/wp-content/uploads/2018/07/stemming-example.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ee5c66",
   "metadata": {},
   "source": [
    "![](https://i0.wp.com/trevorfox.com/wp-content/uploads/2018/07/stemming-example.png?fit=500%2C605&ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d949456e",
   "metadata": {},
   "source": [
    "Comparison with lemmatization. From: https://medium.com/swlh/introduction-to-stemming-vs-lemmatization-nlp-8c69eb43ecfe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999a8d4e",
   "metadata": {},
   "source": [
    "![](https://tse3-mm.cn.bing.net/th/id/OIP-C.2K4VxxRtewNw4iP-Kh5Z7QHaEH?pid=ImgDet&rs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b440ba",
   "metadata": {},
   "source": [
    "Additionally, text enrichment can be applied providing more semantics to the original text with data that we didn't have before. (machine-learning, learn-based)\n",
    "- POS Tag: Part of speech tagging.\n",
    "- Entity Recognition: Recognizing named entities.\n",
    "- Entity relation extraction: Extracting relations between named entities.\n",
    "- Dependency parsing: Parsing the sentence into a tree structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6244d8",
   "metadata": {},
   "source": [
    "POS Tagging. From: https://www.researchgate.net/publication/337460636_Unpacking_the_Smart_Mobility_Concept_in_the_Dutch_Context_Based_on_a_Text_Mining_Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9768b9",
   "metadata": {},
   "source": [
    "![](https://www.researchgate.net/publication/337460636/figure/download/fig1/AS:828223747284992@1574475337385/Example-of-part-of-speech-POS-tagging-and-lemmatization-for-two-example-sentences-The.ppm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a323fdfb",
   "metadata": {},
   "source": [
    "Entity recognition and dependency parsing. From: https://stanfordnlp.github.io/CoreNLP/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd36a084",
   "metadata": {},
   "source": [
    "![](https://stanfordnlp.github.io/CoreNLP/assets/images/ner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7674507",
   "metadata": {},
   "source": [
    "Entity relation-extraction. From: https://www.mdpi.com/2079-9292/9/10/1637"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd36a084",
   "metadata": {},
   "source": [
    "![](https://www.mdpi.com/electronics/electronics-09-01637/article_deploy/html/images/electronics-09-01637-g001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd364f68",
   "metadata": {},
   "source": [
    "Then we have text vectorization, which is the process of converting the text into a vector representation. This step is required to train a machine learning model.\n",
    "\n",
    "Vectorized representations of text are usually obtained via:\n",
    "- Bag of words: A vector representation of the text is obtained by counting the number of times each word appears in the text.\n",
    "- TF-IDF: A vector representation of the text is obtained by counting the number of times each word appears in the text and then normalizing the counts by the number of documents in which the word appears.\n",
    "- Word embeddings: A vector representation of the text is obtained by using a word embedding model to represent the text.\n",
    "    - Word2Vec.\n",
    "    - Bert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3aa398",
   "metadata": {},
   "source": [
    "From here the vectorized representation becomes the input for the machine learning algorithm. Depending of the algorithm, the input can be:\n",
    "- Classification: The input is a vector representation of the text and the output is a class label.\n",
    "- Regression: The input is a vector representation of the text and the output is a real number.\n",
    "- Clustering: The input is a vector representation of the text and the output is a cluster label.\n",
    "- Recommendation: The input is a vector representation of the text and the output is a list of recommendations.\n",
    "- Sentiment analysis: The input is a vector representation of the text and the output is a real number.\n",
    "- Topic modeling: The input is a vector representation of the text and the output is a list of topics.\n",
    "- Text summarization: The input is a vector representation of the text and the output is a list of sentences.\n",
    "- Text translation: The input is a vector representation of the text and the output is a list of translations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f71e40e",
   "metadata": {},
   "source": [
    "### Revisit the previous example\n",
    "\n",
    "Implement a few of the preprocessing steps mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7237d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import requests\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d58e3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dependencies\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5340ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample text, from the provided url into a dataframe\n",
    "response = requests.get('http://www.textfiles.com/stories/alad10.txt')\n",
    "sample_text = response.text\n",
    "sentences = sample_text.split('\\n')                        # Split text into sentences\n",
    "df = pd.DataFrame(sentences, columns=['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47c030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning (morphological changes)\n",
    "df['sentence'] = df['sentence'].str.lower()                 # Lowercase\n",
    "df = df[df['sentence'].str.split().str.len() > 3]           # Remove short sentences\n",
    "df['sentence'] = df['sentence'].str.replace('[^\\w\\s]','')   # Remove punctuation\n",
    "max_len = df['sentence'].str.len().max()                    # longest sentence\n",
    "df['sentence'].head(5)                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa22224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords\n",
    "eng_stopwords = stopwords.words('english')\n",
    "print(eng_stopwords[-10:])\n",
    "df['sentence'] = df['sentence'].apply(lambda x: ' '.join([word for word in x.split() if word not in eng_stopwords]))\n",
    "df['sentence'].head(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cdd321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['sentence'] = df['sentence'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "df['sentence'].head(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fe779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stemming\n",
    "stemmer = PorterStemmer()\n",
    "df['sentence'] = df['sentence'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "df['sentence'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205585c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentences, add tokens ids\n",
    "tokens_df = df.copy()\n",
    "tokens_df['tokenized_sentence'] = tokens_df['sentence'].apply(bert_tokenizer.tokenize)\n",
    "tokens_df['numericalized_sentence'] = tokens_df['tokenized_sentence'].apply(bert_tokenizer.convert_tokens_to_ids)\n",
    "tokens_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dd7f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the [CLS] and [SEP] special tokens and padding to the numericalized sentences on the dataframe\n",
    "tokens_df['numericalized_sentence'] = tokens_df['numericalized_sentence'].apply(lambda x: [bert_tokenizer.cls_token_id] + x + [bert_tokenizer.sep_token_id])\n",
    "tokens_df['numericalized_sentence'] = tokens_df['numericalized_sentence'].apply(lambda x: x + [bert_tokenizer.pad_token_id] * (max_len - len(x)))\n",
    "tokens_df['numericalized_sentence'].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2344b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract encoded value to a Tensor\n",
    "numericalized_sentences = tokens_df['numericalized_sentence'].values\n",
    "numericalized_sentences = [list(x) for x in numericalized_sentences]\n",
    "numericalized_sentences = np.array(numericalized_sentences)\n",
    "numericalized_sentences = torch.from_numpy(numericalized_sentences)\n",
    "print(numericalized_sentences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfdd7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the numericalized sentences using BERT\n",
    "encoded_sentences = bert_model(numericalized_sentences)[0]\n",
    "encoded_sentences = encoded_sentences.detach().numpy()\n",
    "print(encoded_sentences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4384158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add embedings of each sentence\n",
    "encoded_sentences = np.sum(encoded_sentences, axis=1)\n",
    "print(encoded_sentences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9ac81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA to reduce the embedding dimensionality to 3\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(encoded_sentences)\n",
    "reduced_embeddings = pca.transform(encoded_sentences)\n",
    "print(reduced_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77be743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 3D embeddings\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], reduced_embeddings[:, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d52596f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise: experiment!\n",
    "\n",
    "* Combine text from at least two different sources.\n",
    "* Try different nlp libraries\n",
    "* Perform an expanded NLP pipeline (check spelling, POS tagging, entity recognition, dependency parsing, etc.)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b35736f271ae3a6547f08ef3ad12296102f5e6031b2a7c6493a7e5cc9f313275"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
