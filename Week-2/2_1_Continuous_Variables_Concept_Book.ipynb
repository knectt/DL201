{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸ’» UnpackAI DL201 Bootcamp - Week 2 - Concepts: Continuous Variables","metadata":{}},{"cell_type":"markdown","source":"### ðŸ“• Learning Objectives\n<hr style=\"border:2px solid gray\"> </hr>\n","metadata":{}},{"cell_type":"markdown","source":"* Firmly grasp how continuous variables have a mathematical meaning which allows for certain algorithms to use them as input data  \n* Gain an Appreciation for how **scaling** can not only decrease training time, but also increase increase the quality of the input data in both tabular and image data\n* Understand the differences between standardization and normalization, and in which situations to apply them\n* Build awareness of the importance of the normal distribution, and how to transform data using log and boxcox transforms\n* Appreciate how these very same properties can be applied in 2 or more dimensions in image data through broadcasting methods\n\n","metadata":{}},{"cell_type":"markdown","source":"### ðŸ“– Concepts map\n<hr style=\"border:2px solid gray\"> </hr>\n","metadata":{}},{"cell_type":"markdown","source":"* Quantitative vs Qualitative\n* Fundamental Theorem of Calculus\n* Law of Large Numbers/\n* Normal Distribution\n* Standard Deviation\n* Skew\n* Tensor Data Types\n* Garbage in Garbage out\n* Local vs Global Transformation\n","metadata":{}},{"cell_type":"code","source":"# Imports \nfrom pathlib import Path\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport os","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:26:18.459385Z","iopub.execute_input":"2022-07-20T03:26:18.459941Z","iopub.status.idle":"2022-07-20T03:26:19.153094Z","shell.execute_reply.started":"2022-07-20T03:26:18.459821Z","shell.execute_reply":"2022-07-20T03:26:19.152173Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"is_kaggle = True   # True if you are on Kaggle, False for local Windows, Linux or Mac environments.","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:26:21.316371Z","iopub.execute_input":"2022-07-20T03:26:21.317369Z","iopub.status.idle":"2022-07-20T03:26:21.323049Z","shell.execute_reply.started":"2022-07-20T03:26:21.317300Z","shell.execute_reply":"2022-07-20T03:26:21.321934Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# if is_kaggle:\n    \n#     IMAGE_DIR = Path('/kaggle/working/DL201/img')\n#     DATA_DIR = Path('/kaggle/working/DL201/data')\n","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:26:52.412779Z","iopub.execute_input":"2022-07-20T03:26:52.413217Z","iopub.status.idle":"2022-07-20T03:26:52.418843Z","shell.execute_reply.started":"2022-07-20T03:26:52.413187Z","shell.execute_reply":"2022-07-20T03:26:52.417775Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# path preparation\nif is_kaggle:\n    !git clone https://github.com/unpackAI/DL201.git\n    IMAGE_DIR = Path('/kaggle/working/DL201/img')\n    DATA_DIR = Path('/kaggle/working/DL201/data')\n    OUTPUT_DIR='./output/'\nelse:\n    # This section is for local execution, it is assumed that we launch the notebooks from the DL201 repository.\n    DATA_DIR = Path('../data')\n    IMAGE_DIR = Path('../img')\n    OUTPUT_DIR = Path('../output')\n\n# finally, check if we found the right pathes\nif os.path.isdir(DATA_DIR):\n    print(f'DATA_DIR is a directory, its path is {DATA_DIR}')\nelse:\n    print(\"ERROR : DATA_DIR is not a directory\")\n\nif os.path.isdir(IMAGE_DIR):\n    print(f'IMAGE_DIR is a directory, its path is {IMAGE_DIR}')\nelse:\n    print(\"ERROR : IMAGE_DIR is not a directory\")\n\n# create an output directory if necessary\nif not os.path.isdir(OUTPUT_DIR):\n    os.mkdir(OUTPUT_DIR)\nif os.path.isdir(OUTPUT_DIR):\n    print(f'OUTPUT_DIR is a directory, its path is {OUTPUT_DIR}')\nelse:\n    print(\"ERROR : OUTPUT_DIR is not a directory\")","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:27:23.629792Z","iopub.execute_input":"2022-07-20T03:27:23.630226Z","iopub.status.idle":"2022-07-20T03:27:30.953824Z","shell.execute_reply.started":"2022-07-20T03:27:23.630192Z","shell.execute_reply":"2022-07-20T03:27:30.952431Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Splits columns into categorical and continious variables \ndef cont_cat_split(df, max_card=20, dep_var=None):\n    \"Helper function that returns column names of cont and cat variables from given `df`.\"\n    cont_names, cat_names = [], []\n    for label in df:\n        #if label in L(dep_var): continue\n        if ((pd.api.types.is_integer_dtype(df[label].dtype) and\n            df[label].unique().shape[0] > max_card) or\n            pd.api.types.is_float_dtype(df[label].dtype)):\n            cont_names.append(label)\n        else: cat_names.append(label)\n    return cont_names, cat_names","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:27:56.894268Z","iopub.execute_input":"2022-07-20T03:27:56.894915Z","iopub.status.idle":"2022-07-20T03:27:56.904746Z","shell.execute_reply.started":"2022-07-20T03:27:56.894858Z","shell.execute_reply":"2022-07-20T03:27:56.903534Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## What is a Continuous Variable?\n<hr style=\"border:2px solid gray\"> </hr>\n","metadata":{}},{"cell_type":"markdown","source":"There are primarily two kinds of variables in statistics, being continuous and categorical. Continuous variables are quantifiable numbers that exist on a spectrum. An ideal continuous variable can be any number or decimal between the minimum and maximum. For example, measurements based in units such as grams, meters, and liters, temperature ect. meet this criteria. \n\nHowever, it gets a bit fuzzy when you have real world data. However, there's enough data, it's generally accepted to treat anything that can exist on an ordered spectrum, has many possible states and has mathematical meaning as a continuous variable. For example, financial data such as revenue, profits, number of items sold, or price are continuous variables.\n\nAnother question here comes down to where an image fits into this. Because there are many different possible values in a pixel, and many thousands of pixels in each image, it becomes possible to treat an image as a continuous variable. Because they have these same, base, numerical properties, it is possible to extend the same techniques done across columns in statistics and broadcast them in 2 or 3 dimensions. ","metadata":{}},{"cell_type":"markdown","source":"### A few examples of continuous variables\n\n* Weight\n* Running Distance\n* Revenue\n* Number of Tickets Sold\n\n* Grayscale Image\n* RGB Image","metadata":{}},{"cell_type":"code","source":"houses_df = pd.read_csv(DATA_DIR/'house-prices'/'train.csv',index_col=0)","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:28:03.357110Z","iopub.execute_input":"2022-07-20T03:28:03.357588Z","iopub.status.idle":"2022-07-20T03:28:03.401167Z","shell.execute_reply.started":"2022-07-20T03:28:03.357548Z","shell.execute_reply":"2022-07-20T03:28:03.400091Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"cont_vars, cat_vars = cont_cat_split(houses_df)","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:28:04.688283Z","iopub.execute_input":"2022-07-20T03:28:04.689373Z","iopub.status.idle":"2022-07-20T03:28:04.708090Z","shell.execute_reply.started":"2022-07-20T03:28:04.689314Z","shell.execute_reply":"2022-07-20T03:28:04.706970Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Here are the continuous variables in this dataset.","metadata":{}},{"cell_type":"code","source":"print(cont_vars)","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:28:09.002592Z","iopub.execute_input":"2022-07-20T03:28:09.003541Z","iopub.status.idle":"2022-07-20T03:28:09.010534Z","shell.execute_reply.started":"2022-07-20T03:28:09.003487Z","shell.execute_reply":"2022-07-20T03:28:09.008881Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"If you wish to swap one of the features of your dataset, you can use this function to do so.","metadata":{}},{"cell_type":"code","source":"def change_feature_type(swapFeature,cat_vars,cont_vars):\n    cat_df = pd.DataFrame(cat_vars)\n    cont_df = pd.DataFrame(cont_vars)\n\n    cat_result = cat_df[cat_df[0]==swapFeature]\n    cont_result = cont_df[cont_df[0]==swapFeature]\n\n    if len(cat_result) > 0:\n        print(f'Found: {swapFeature} in catagorical varaibles, swapping to continous variable')\n        cont_vars.append(swapFeature)\n        cat_vars.remove(swapFeature)\n\n    elif len(cont_result) > 0:\n        print(f'Found: {swapFeature} in continuous variables, swapping to catagorical variable')\n        cat_vars.append(swapFeature)\n        cont_vars.remove(swapFeature)\n\n    else:\n        print(f\"Feature: {swapFeature} was not found in either list, please check spelling\")  \n    return (cat_vars,cont_vars)","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:28:11.657016Z","iopub.execute_input":"2022-07-20T03:28:11.658182Z","iopub.status.idle":"2022-07-20T03:28:11.667040Z","shell.execute_reply.started":"2022-07-20T03:28:11.658133Z","shell.execute_reply":"2022-07-20T03:28:11.664503Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#swapThisFeature = '' #'Column_Name'\n\n#cat_vars, cont_vars = change_feature_type(swapThisFeature,cat_vars,cont_vars)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:28:15.680773Z","iopub.execute_input":"2022-07-20T03:28:15.681209Z","iopub.status.idle":"2022-07-20T03:28:15.686721Z","shell.execute_reply.started":"2022-07-20T03:28:15.681176Z","shell.execute_reply":"2022-07-20T03:28:15.685290Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## How can Machine Learning and Deep Learning Use Them?\n<hr style=\"border:2px solid gray\"> </hr>\n","metadata":{}},{"cell_type":"markdown","source":"As a result of being numbers, they have some amazing properties that form the foundations which science, and ultimately many Machine Learning and Deep Learning models are built on. ","metadata":{}},{"cell_type":"markdown","source":"### Mathematical Meaning\n\n","metadata":{}},{"cell_type":"markdown","source":"For this, at a high level, there are two very critical properties of continous numbers that models use we are going to talk about.\n\n* One is the the slope of a line, which comes from methods of calculus.\n\n* The Second is distance, which comes from geometry. \n\nThe first point about the slope of a line (or gradient 2D+) gives us the parameters in regression based models, SGD (Stochastic Grade Descent) ect.\n\nThe second property of distance we can use, allows us to create a relationship between different values using the concept of distance (Euclidean Distance).\n\nAt this point, we're note getting into how the models work, but rather going over just enough to understand the reasons why they need to be preprocessed to get the best results out of a Machine Learning or Deep Learning Model. \n\n","metadata":{}},{"cell_type":"markdown","source":"## Why do We Need to Scale Continuous Variables?\n<hr style=\"border:2px solid gray\"> </hr>\n","metadata":{}},{"cell_type":"markdown","source":"### Many AI Algorithms Train Faster\n<hr style=\"border:2px solid gray\"> </hr>\n","metadata":{}},{"cell_type":"markdown","source":"Real world data can have many different forms. There could be continuous variables that have only small differences between the largest and smallest value, or there can be a range of millions. \n\nBut, because there are numbers involved, it's not so much the actual values that are important, but the relationship between them. Scaling not only makes the models have to work less hard, but extracts the key relationships in the data which actually are important and puts them onto a level playing field.","metadata":{}},{"cell_type":"markdown","source":"#### Models that use Distance as a measure","metadata":{}},{"cell_type":"markdown","source":"Models such as K-Means or K-Nearest Neighbors use the concept of distance to extract information from the features. \n\nIf the different features have very different ranges of values, then not only does the model have to train longer to get the relationships, but the distances are relative only to themselves rather than across the dataset. \n\nFor example, if we had a distance of meters walked by an ant, and a distance of kilometers walked by an elephant, then the distance the elephant traveled is much greater, even if the ant traveled much further relative to it's body size. ","metadata":{}},{"cell_type":"markdown","source":"#### Models that calculate gradients/slopes","metadata":{}},{"cell_type":"markdown","source":"Other models, such as SGD (Stochastic Grade Descent) use the concept of slope to optimize the performance of the model. When the values are normalized within a same range, it trains faster than having some parameters as huge, than other parameters as very small.","metadata":{}},{"cell_type":"markdown","source":"### Allows model to become more complex\n<hr style=\"border:2px solid gray\"> </hr>\n\nIn Deep Learning there can be thousands upon thousands of parameters in a model. As a result, some of the parameters can become quite tiny because they all need to add up to a small value.\n\nIf some of the features are larger than others, then it drowns out the importance of smaller valued features. By having scaled values, we can increase the complexity of the model\n\n","metadata":{}},{"cell_type":"markdown","source":"Scaling continuous variables is critical not only to save computer resources, but also to improve the accuracy of the model. Many AutoML tools do this automatically, but it is important to understand the concepts so that they are applied appropriately.","metadata":{}},{"cell_type":"markdown","source":"# Section 1: Scaling Tabular Data \n<hr style=\"border:4px solid gray\"> </hr>\n","metadata":{}},{"cell_type":"markdown","source":"In this notebook, we'll start with Tabular Data because we can treat it as 1D, then build up the knowledge base so that it can be applied to images as well.","metadata":{}},{"cell_type":"markdown","source":"### Normalization \n<hr style=\"border:2px solid gray\"> </hr>\n","metadata":{}},{"cell_type":"markdown","source":"#### Min Max Scaling \n\nOne problem that could result from having a large range of data is that the large numbers and variance can cause the model to see it as more important than variables that have a smaller variance. \n\nNormalization can put the values into a range that falls between zero and one\n\nHere is this way of scaling : substract the smallest value x_min to each value x. This will let the new values start from 0. Then, you will divide those differences by the difference between the largest and smallest elements x_max - x_min.\n\n$$ \\tilde{x}_i = \\frac{x_i - x_{min}}{x_{max} - x_{min}}. $$\n\nA year column would benefit from this kind of transformation. Since we are in the 21st century, everything in this column is going to have a huge offset of 2000\n\n### Question : \ncould you give an example when this formula does not give good results ?\n\nBelow, let's see how it can transform the data","metadata":{}},{"cell_type":"code","source":"houses_df['YearBuilt'].mean()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:28:26.601189Z","iopub.execute_input":"2022-07-20T03:28:26.601650Z","iopub.status.idle":"2022-07-20T03:28:26.612154Z","shell.execute_reply.started":"2022-07-20T03:28:26.601598Z","shell.execute_reply":"2022-07-20T03:28:26.610844Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"sns.histplot(houses_df['YearBuilt'])","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:28:28.697883Z","iopub.execute_input":"2022-07-20T03:28:28.698556Z","iopub.status.idle":"2022-07-20T03:28:28.966685Z","shell.execute_reply.started":"2022-07-20T03:28:28.698518Z","shell.execute_reply":"2022-07-20T03:28:28.965446Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"In a large model, this is a problem, because it will change the parameter associated with the house prices to assume that the number will be very large relative to the other ones.  ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nmin_max_scaler = MinMaxScaler()\n\ndf = houses_df\n\ncolumn_names_to_normalize = ['YearBuilt','YearRemodAdd','GarageYrBlt']\n\nx = df[column_names_to_normalize].values\nx_scaled = min_max_scaler.fit_transform(x)\n\ndf_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = df.index)\ndf[column_names_to_normalize] = df_temp\n\nhouses_df = df.copy()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:28:36.109296Z","iopub.execute_input":"2022-07-20T03:28:36.109953Z","iopub.status.idle":"2022-07-20T03:28:36.187142Z","shell.execute_reply.started":"2022-07-20T03:28:36.109906Z","shell.execute_reply":"2022-07-20T03:28:36.185939Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"houses_df['YearBuilt'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:28:38.764827Z","iopub.execute_input":"2022-07-20T03:28:38.766198Z","iopub.status.idle":"2022-07-20T03:28:38.780237Z","shell.execute_reply.started":"2022-07-20T03:28:38.766147Z","shell.execute_reply":"2022-07-20T03:28:38.779168Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"sns.histplot(houses_df['YearBuilt'])","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:28:39.267732Z","iopub.execute_input":"2022-07-20T03:28:39.268587Z","iopub.status.idle":"2022-07-20T03:28:39.489319Z","shell.execute_reply.started":"2022-07-20T03:28:39.268539Z","shell.execute_reply":"2022-07-20T03:28:39.488165Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Although it is no longer meaningful to a person because we've lost the dates and our associations with them, it is much more clear to a machine because all the values are between zero and one. \n\nThis transform does not change the ratios of values to eachother. The two graphs look identical, except for now that they have different values. ","metadata":{}},{"cell_type":"markdown","source":"Why is the skew unaffected by this transform? What information does this transform preserve?","metadata":{}},{"cell_type":"code","source":"houses_df['YearBuilt'].skew()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:28:44.324298Z","iopub.execute_input":"2022-07-20T03:28:44.324843Z","iopub.status.idle":"2022-07-20T03:28:44.333601Z","shell.execute_reply.started":"2022-07-20T03:28:44.324794Z","shell.execute_reply":"2022-07-20T03:28:44.332366Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"You do not know what is a the skew of a distribution ? The following page explains it very well : https://www.mathsisfun.com/data/skewness.html","metadata":{}},{"cell_type":"markdown","source":"Generally speaking, standardization should be used when your model has a regularization term or is otherwise sensitive to the scaling of the input features. Standardization transforms all features onto the same scaling, thereby ensuring that regularization and other scaling-sensitive operations work properly.","metadata":{}},{"cell_type":"markdown","source":"### Standardization \n<hr style=\"border:2px solid gray\"> </hr>\n","metadata":{}},{"cell_type":"markdown","source":"Problems with large scales of numbers can be that it is not easy to see a pattern if there are a huge range of possible values.\n\nThis is where the normal distribution comes in. A normal distribution is an incredibly important property of large samples that allows a class of machine learning algorithms to work.\n\nThey do this because a probability value can be assigned to a sample. Based on ***how far it is from the mean***, assumptions can be made about how likely this value is to appear.\n\nIf they are standardized, we are no longer looking at the raw value. But instead looks at how many standard deviations it is from the mean.\n\nAgain, it is not the individual numbers that are important, but the relationship between them which really matters. Standardization shows this to the model more clearly.\n\nhttps://www.kdnuggets.com/2020/04/data-transformation-standardization-normalization.html","metadata":{}},{"cell_type":"markdown","source":"### Skewed Data\n\nThe problem is that data often comes in a skewed form. This can be corrected by various transforms. \n\nAlthough there is no consensus on the threshold of what constitutes skewed data, but for the purposes of this notebook, a skew value above 1 is considered skewed. ","metadata":{}},{"cell_type":"code","source":"sns.histplot(houses_df['SalePrice'])\nprint('Skew of Sales Price Distribution: ', houses_df['SalePrice'].skew())","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:28:51.703569Z","iopub.execute_input":"2022-07-20T03:28:51.704615Z","iopub.status.idle":"2022-07-20T03:28:51.983147Z","shell.execute_reply.started":"2022-07-20T03:28:51.704551Z","shell.execute_reply":"2022-07-20T03:28:51.982023Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"This looks like a very nice graph, but it has a real problem. There is a strong skew to the left with an appreciable tail. As you can see below, the skew value is quite large","metadata":{}},{"cell_type":"code","source":"skewed_features = houses_df[cont_vars].skew()\nsorted_skewed_features = skewed_features.sort_values(ascending=False) # for information\nskewed_features_series = pd.Series(skewed_features)\n\nPositiveSkewedFeatures = skewed_features_series[skewed_features_series > 1]\nNegativeSkewedFeatures = skewed_features_series[skewed_features_series < -1]\nnormalFeatures = skewed_features_series[skewed_features_series.abs() <= 1]","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:28:55.892030Z","iopub.execute_input":"2022-07-20T03:28:55.893419Z","iopub.status.idle":"2022-07-20T03:28:55.906563Z","shell.execute_reply.started":"2022-07-20T03:28:55.893358Z","shell.execute_reply":"2022-07-20T03:28:55.905388Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"sorted_skewed_features","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:28:59.172106Z","iopub.execute_input":"2022-07-20T03:28:59.173335Z","iopub.status.idle":"2022-07-20T03:28:59.181478Z","shell.execute_reply.started":"2022-07-20T03:28:59.173285Z","shell.execute_reply":"2022-07-20T03:28:59.180356Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"print('Number of Negative Skewed Features: ',len(NegativeSkewedFeatures))\nprint('Number of Positive Skewed Features: ',len(PositiveSkewedFeatures))\nprint('Number of not Skewed Features: ',len(normalFeatures))","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:29:04.264197Z","iopub.execute_input":"2022-07-20T03:29:04.264673Z","iopub.status.idle":"2022-07-20T03:29:04.271743Z","shell.execute_reply.started":"2022-07-20T03:29:04.264613Z","shell.execute_reply":"2022-07-20T03:29:04.270493Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"These statistics, among many others can rapidly and easily be obtained by pandas methods and a quick internet search.\n\nIn this notebook, we don't go into kurtosis but this is also available in the pandas library : https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.kurtosis.html.","metadata":{}},{"cell_type":"markdown","source":"#### Log Transforms \n\nLog transforms help to normalize data significantly. This is useful in positive skew distributions","metadata":{}},{"cell_type":"code","source":"houses_df['SalePrice'] = np.log(houses_df['SalePrice'])\n","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:29:09.188655Z","iopub.execute_input":"2022-07-20T03:29:09.189082Z","iopub.status.idle":"2022-07-20T03:29:09.196279Z","shell.execute_reply.started":"2022-07-20T03:29:09.189049Z","shell.execute_reply":"2022-07-20T03:29:09.195260Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"sns.histplot(houses_df['SalePrice'])","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:29:10.157070Z","iopub.execute_input":"2022-07-20T03:29:10.157532Z","iopub.status.idle":"2022-07-20T03:29:10.404006Z","shell.execute_reply.started":"2022-07-20T03:29:10.157491Z","shell.execute_reply":"2022-07-20T03:29:10.402939Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"As you can see, this graph is now looking much more like a normal distribution. It has a skew much closer to zero, which means that we can use linear models, and the model will train faster.","metadata":{}},{"cell_type":"code","source":"houses_df['SalePrice'].skew()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:29:13.470007Z","iopub.execute_input":"2022-07-20T03:29:13.470485Z","iopub.status.idle":"2022-07-20T03:29:13.478153Z","shell.execute_reply.started":"2022-07-20T03:29:13.470446Z","shell.execute_reply":"2022-07-20T03:29:13.476823Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"#### Boxcox Transform\n\nThe Boxcox Transform is another method in the scipy library that usually performs better than log transforms. It is a commonly used tool to normalize skewed data","metadata":{}},{"cell_type":"code","source":"#stats\nfrom scipy import stats\n\ndf = houses_df.copy()\n\nfor col in PositiveSkewedFeatures.index:\n    #print(col)\n    original_skew = round(df[col].skew(),2)\n    \n    #np.clip replaces zeros with tiny numbers\n    # to avoid how it's impossible to take\n    # the log of zero\n    \n    transformed_col = stats.boxcox(np.clip(df[col],0.0001,None))[0]\n    \n    tsfm_skew = round(pd.Series(transformed_col).skew(),2)\n    \n    if abs(tsfm_skew) < abs(original_skew):\n        df[col] = transformed_col\n        print(f'{col} skewed decreased from {original_skew} to {tsfm_skew}')\n    else:\n        pass\n\n\nhouses_df = df","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:29:16.732795Z","iopub.execute_input":"2022-07-20T03:29:16.733229Z","iopub.status.idle":"2022-07-20T03:29:16.814155Z","shell.execute_reply.started":"2022-07-20T03:29:16.733194Z","shell.execute_reply":"2022-07-20T03:29:16.813191Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nstandard_scalar = StandardScaler()\n\ndf = houses_df\n\ncolumns_to_standardize = cont_vars\n\nx = df[columns_to_standardize].values\n\nx_scaled = standard_scalar.fit_transform(x)\ndf_temp = pd.DataFrame(x_scaled, columns=columns_to_standardize, index = df.index)\ndf[columns_to_standardize] = df_temp\n\nhouses_df = df","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:29:23.164427Z","iopub.execute_input":"2022-07-20T03:29:23.165220Z","iopub.status.idle":"2022-07-20T03:29:23.178000Z","shell.execute_reply.started":"2022-07-20T03:29:23.165180Z","shell.execute_reply":"2022-07-20T03:29:23.177031Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Section 2: Scaling Image Data\n<hr style=\"border:4px solid gray\"> </hr>\n","metadata":{}},{"cell_type":"markdown","source":"Now that we have done through scaling 1D tabular data, it is only a matter of applying an understanding of broadcasting rules to  2D and 3D data. \n\n\nBecause the normal distribution is so pervasive, it is only natural that it would appear in images as well. Images also often have a normal distribution of pixel values.\n","metadata":{}},{"cell_type":"markdown","source":"### Why do images need to be scaled?\n<hr style=\"border:2px solid gray\"> </hr>\n","metadata":{}},{"cell_type":"markdown","source":"Secondly, scaling becomes even more important for images going into Neural Networks. Neural Networks can have millions of parameters, and this can also affect the magnitude of the loss functions, and activation functions. It's easier for networks to learn when the data is scaled to zero mean, or between 0-1. ","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nfrom matplotlib import pyplot as plt\n\nimagePath = IMAGE_DIR/'week2'/'goldengatebridge.jpg'\n\nimage = Image.open(imagePath)","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:29:26.565147Z","iopub.execute_input":"2022-07-20T03:29:26.566001Z","iopub.status.idle":"2022-07-20T03:29:26.571574Z","shell.execute_reply.started":"2022-07-20T03:29:26.565954Z","shell.execute_reply":"2022-07-20T03:29:26.570821Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# It's always a good idea to check the type\nplt.imshow(image)\nplt.show()\ntype(image)","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:29:29.830521Z","iopub.execute_input":"2022-07-20T03:29:29.831660Z","iopub.status.idle":"2022-07-20T03:29:30.191441Z","shell.execute_reply.started":"2022-07-20T03:29:29.831582Z","shell.execute_reply":"2022-07-20T03:29:30.190693Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"This image has many colors in it and is a good candidate to test using what we learned about tabular data and applying to higher dimensional data. Let's first represent it as a 3D array","metadata":{}},{"cell_type":"code","source":"rgb_array = np.asarray(image)","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:29:33.775743Z","iopub.execute_input":"2022-07-20T03:29:33.776642Z","iopub.status.idle":"2022-07-20T03:29:33.786835Z","shell.execute_reply.started":"2022-07-20T03:29:33.776585Z","shell.execute_reply":"2022-07-20T03:29:33.785711Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"#always check the shape \nrgb_array.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:29:34.914812Z","iopub.execute_input":"2022-07-20T03:29:34.915266Z","iopub.status.idle":"2022-07-20T03:29:34.922353Z","shell.execute_reply.started":"2022-07-20T03:29:34.915232Z","shell.execute_reply":"2022-07-20T03:29:34.921102Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"Most images have pixel values between 0 and 255.\n\nSince we are no longer using it to display it for a person to read, we should change its representation to make the mathematical meaning clearer. This not only speeds up the training process, but eliminates possible complications that become difficult to check out once the model is trained because there are many different parameters.\n\nScaling all values to be between between 0-1 accomplishes this. ","metadata":{}},{"cell_type":"code","source":"# This allows us to make decimal values\nrgb_array = rgb_array.astype('float32')","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:29:39.773674Z","iopub.execute_input":"2022-07-20T03:29:39.774113Z","iopub.status.idle":"2022-07-20T03:29:39.783835Z","shell.execute_reply.started":"2022-07-20T03:29:39.774073Z","shell.execute_reply":"2022-07-20T03:29:39.782748Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"scaled_array = rgb_array / 255.0","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:29:40.932236Z","iopub.execute_input":"2022-07-20T03:29:40.933666Z","iopub.status.idle":"2022-07-20T03:29:40.944102Z","shell.execute_reply.started":"2022-07-20T03:29:40.933594Z","shell.execute_reply":"2022-07-20T03:29:40.942797Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"print(f'before scaling : min : {rgb_array.min()}  max : {rgb_array.max()}')","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:29:43.093721Z","iopub.execute_input":"2022-07-20T03:29:43.094543Z","iopub.status.idle":"2022-07-20T03:29:43.102841Z","shell.execute_reply.started":"2022-07-20T03:29:43.094498Z","shell.execute_reply":"2022-07-20T03:29:43.101618Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"print(f'after scaling : min : {scaled_array.min()}  max : {scaled_array.max()}')","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:29:46.209064Z","iopub.execute_input":"2022-07-20T03:29:46.209523Z","iopub.status.idle":"2022-07-20T03:29:46.217341Z","shell.execute_reply.started":"2022-07-20T03:29:46.209482Z","shell.execute_reply":"2022-07-20T03:29:46.216558Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"Scaling is the best way to prepare data which you are not sure how to preprocess. It preserves all the quantifiable information in the dataset, and doesn't require any statistics to do.","metadata":{}},{"cell_type":"markdown","source":"### Image Standardization\n<hr style=\"border:2px solid gray\"> </hr>\n","metadata":{}},{"cell_type":"markdown","source":"Often, the distribution of pixels in an image will follow a normal distribution (bell curve).\n\nThis may be present across the entire dataset, or in batches of images, which allows for the transformation to be done in batches on a GPU very quickly.\n\nHowever, in this example, we will just standardize one image so that we can form a base of understanding.","metadata":{}},{"cell_type":"code","source":"# calculate global mean and standard deviation\nglobal_pixel_mean = rgb_array.mean()\nprint('Mean of all Pixels: ', global_pixel_mean)\n\nglobal_pixel_std = rgb_array.std()\nprint('standard deviation of all pixels: ',global_pixel_std)\n\n# global standardization of pixels\nstandardized_array = (rgb_array - global_pixel_mean) / global_pixel_std\n# values before -1 will be set to -1, values above 1 will be set to 1\nstandardized_array = np.clip(standardized_array, -1.0, 1.0)","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:29:53.147620Z","iopub.execute_input":"2022-07-20T03:29:53.148108Z","iopub.status.idle":"2022-07-20T03:29:53.177595Z","shell.execute_reply.started":"2022-07-20T03:29:53.148067Z","shell.execute_reply":"2022-07-20T03:29:53.176327Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# check the mean and standard deviation after standardization\nstandardized_pixel_mean = standardized_array.mean()\nprint('Mean of all Pixels: ', standardized_pixel_mean)\n\nstandardized_pixel_std = standardized_array.std()\nprint('standard deviation of all pixels: ',standardized_pixel_std)","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:29:58.279962Z","iopub.execute_input":"2022-07-20T03:29:58.280418Z","iopub.status.idle":"2022-07-20T03:29:58.294717Z","shell.execute_reply.started":"2022-07-20T03:29:58.280381Z","shell.execute_reply":"2022-07-20T03:29:58.293160Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"### Local Standardization\n<hr style=\"border:1px solid gray\"> </hr>","metadata":{}},{"cell_type":"markdown","source":"It is also possible to standardize each channel invididually rather than across the whole image","metadata":{}},{"cell_type":"code","source":"rgb_means = rgb_array.mean(axis=(0,1), dtype='float64')\nrgb_stds = rgb_array.std(axis=(0,1), dtype='float64')\n\nprint('Means: %s, Stds: %s' % (rgb_means, rgb_stds))\n# per-channel standardization of pixels\n\nstandardized_array_3_channel = (rgb_array - rgb_means) / rgb_stds","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:30:15.078380Z","iopub.execute_input":"2022-07-20T03:30:15.078880Z","iopub.status.idle":"2022-07-20T03:30:15.203365Z","shell.execute_reply.started":"2022-07-20T03:30:15.078841Z","shell.execute_reply":"2022-07-20T03:30:15.201957Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# check the mean and standard deviation after standardization\nrgb_means_after = standardized_array_3_channel.mean(axis=(0,1), dtype='float64')\nrgb_stds_after = standardized_array_3_channel.std(axis=(0,1), dtype='float64')\n\nprint('Means: %s, Stds: %s' % (rgb_means_after, rgb_stds_after))","metadata":{"execution":{"iopub.status.busy":"2022-07-20T03:30:16.869451Z","iopub.execute_input":"2022-07-20T03:30:16.870880Z","iopub.status.idle":"2022-07-20T03:30:16.944587Z","shell.execute_reply.started":"2022-07-20T03:30:16.870825Z","shell.execute_reply":"2022-07-20T03:30:16.943315Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"Now, the values are much more tightly packed than before, which allows for the model to see patterns more easily because the values are within well defined ranges.","metadata":{}},{"cell_type":"markdown","source":"# Wrap up: Discussion\n<hr style=\"border:4px solid gray\"> </hr>\n","metadata":{}},{"cell_type":"markdown","source":"### Extension Questions \n\n* What is a continuous variable? Why are they special?\n\n* What is a normal distribution? What is skew?\n\n* What is min-max scaling? Why is it used with tensors?\n\n* What is standardization? What does it do to data?\n\n* When should one use standardization and when should one use normalization?","metadata":{}}]}