{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üíª UnpackAI DL201 Bootcamp - Week 1 - Skills: Computer Vision\n\n## üìï Learning Objectives\n\n* Loading data\n* Unzipping data\n* Retrieving label information\n* AI model training and results analysis on Computer Vision","metadata":{}},{"cell_type":"markdown","source":"# Part 0 : Code preparation","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport cv2 \nimport torch as t\nfrom matplotlib import pyplot as plt\nfrom pathlib import Path\nimport os\nfrom shutil import unpack_archive\n\n# import data and images if necessary, and choose the right path\nPLATFORM = 'Kaggle'  # or 'UNIX' or 'Kaggle'\n\nif PLATFORM == 'Kaggle':\n    !pip install openpyxl\n    !git clone https://github.com/unpackAI/DL201.git\n    IMAGE_DIR = Path('/kaggle/working/DL201/img')\n    DATA_DIR = Path('/kaggle/working/DL201/data')\n    \nelif PLATFORM == 'UNIX':\n    # we assume that you already cloned unpackAI/DL201 git repository\n    # and this notebook is part of DL201/week1 directory\n    DATA_DIR = Path.home()/'Datasets/unpackAI/DL201/data'\n    IMAGE_DIR = Path.home()/'Projects/unpackAI/DL201/img'\n    \nelif PLATFORM == 'WINDOWS':\n    # we assume that you already cloned unpackAI/DL201 git repository\n    # and this notebook is part of DL201/week1 directory\n    DATA_DIR = Path('../data') #uncomment for kaggle\n    IMAGE_DIR = Path('../img') #Uncomment for Kaggle\n    \n# finally, check if we found the right pathes\nif os.path.isdir(DATA_DIR):\n    print(f'DATA_DIR is a directory, its path is {DATA_DIR}')\nelse:\n    print(\"ERROR : DATA_DIR is not a directory\")\n\nif os.path.isdir(IMAGE_DIR):\n    print(f'IMAGE_DIR is a directory, its path is {IMAGE_DIR}')\nelse:\n    print(\"ERROR : IMAGE_DIR is not a directory\")","metadata":{"execution":{"iopub.status.busy":"2022-05-15T16:07:21.952567Z","iopub.execute_input":"2022-05-15T16:07:21.953253Z","iopub.status.idle":"2022-05-15T16:07:40.859325Z","shell.execute_reply.started":"2022-05-15T16:07:21.953218Z","shell.execute_reply":"2022-05-15T16:07:40.857879Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting openpyxl\n  Downloading openpyxl-3.0.9-py2.py3-none-any.whl (242 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m242.2/242.2 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting et-xmlfile\n  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\nInstalling collected packages: et-xmlfile, openpyxl\nSuccessfully installed et-xmlfile-1.1.0 openpyxl-3.0.9\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCloning into 'DL201'...\nremote: Enumerating objects: 821, done.\u001b[K\nremote: Counting objects: 100% (96/96), done.\u001b[K\nremote: Compressing objects: 100% (59/59), done.\u001b[K\nremote: Total 821 (delta 57), reused 61 (delta 37), pack-reused 725\u001b[K\nReceiving objects: 100% (821/821), 50.59 MiB | 30.72 MiB/s, done.\nResolving deltas: 100% (479/479), done.\nDATA_DIR is a directory, its path is /kaggle/working/DL201/data\nIMAGE_DIR is a directory, its path is /kaggle/working/DL201/img\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Part 1: Loading the data\n<hr style=\"border:4px solid gray\"> </hr>","metadata":{}},{"cell_type":"markdown","source":"As always, the question becomes, how do we access our data.","metadata":{}},{"cell_type":"markdown","source":"## 1.1 How Large is the dataset?\n\nBefore downloading the data, it is a good idea to know how large the dataset is because this will affect how you move forward. If it is very large, you may need to consider if you can download it or not, how much time this would take, how it will be stored in your computer, etc.\n\nYou may need to select a sample out of the dataset to work with rather than work with the whole set. This will speed up exploring the data because you won't constantly be waiting for the computer to process the data.\n\nThis information is usually found online, in the Readme following the data or displayed by your computer when you start downloading.","metadata":{}},{"cell_type":"markdown","source":"## 1.2 Working with compressed files?","metadata":{}},{"cell_type":"markdown","source":"***Zip*** files are incredibly common in many areas.\n\nFor datasets, they serve two primary purposes:\n\n1. This format bundles together many files into one and makes it easier and faster to send it over the internet. Network protocols are similar to the mail. It's much less complicated to send a shipping container rather than do paperwork and handling of thousands of individual boxes.\n\n2. Compression. The other problem is bandwidth. Zip files, along with other formats, can make files smaller which is beneficial because they take up less space on the hard drive. More importantly, this means that we can download the dataset faster. ","metadata":{"toc-hr-collapsed":true}},{"cell_type":"markdown","source":"### Step 1: Find the exact file path","metadata":{}},{"cell_type":"code","source":"# Setting the file path using pathlib\nemotionsImagesZipPath = DATA_DIR/'CV'/'Emotions_Images_Sample.zip'\nos.path.isfile(emotionsImagesZipPath)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T16:07:49.151200Z","iopub.execute_input":"2022-05-15T16:07:49.151547Z","iopub.status.idle":"2022-05-15T16:07:49.161111Z","shell.execute_reply.started":"2022-05-15T16:07:49.151509Z","shell.execute_reply":"2022-05-15T16:07:49.160269Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"### Step 2: Unzip the files\n\nIn this code, we are using a library called ***shutil***\nShutil is short for shell utility.\n\nThis allows python to make commands in the shell.\n\nIn this case, we are telling it to unzip a file X and put it in Y directory.\nX is a file path while Y is a directory path.","metadata":{}},{"cell_type":"code","source":"Output_Directory = os.path.join(DATA_DIR,'OutputDirectory')\nif not os.path.isdir(Output_Directory): #this allows us to run the cell several times\n    os.mkdir(Output_Directory)\n    unpack_archive(emotionsImagesZipPath, Output_Directory)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T16:07:51.359188Z","iopub.execute_input":"2022-05-15T16:07:51.359481Z","iopub.status.idle":"2022-05-15T16:07:51.417586Z","shell.execute_reply.started":"2022-05-15T16:07:51.359453Z","shell.execute_reply":"2022-05-15T16:07:51.416873Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Step 3: Check the file path\n\nNow that we extracted the data, we now need to make sure that we know where our data is, and check up on it before proceeding to the next level.\n\nCheck the output of the next command and see what it is doing.","metadata":{}},{"cell_type":"code","source":"os.listdir(Output_Directory)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T16:07:55.045184Z","iopub.execute_input":"2022-05-15T16:07:55.045835Z","iopub.status.idle":"2022-05-15T16:07:55.051551Z","shell.execute_reply.started":"2022-05-15T16:07:55.045796Z","shell.execute_reply":"2022-05-15T16:07:55.050781Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"['Emotions_Images_Sample']"},"metadata":{}}]},{"cell_type":"code","source":"EMOTIONS_IMAGES_DIR = os.path.join(Output_Directory,'Emotions_Images_Sample')\nlabels = os.listdir(EMOTIONS_IMAGES_DIR)\nprint(labels)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T16:07:57.329380Z","iopub.execute_input":"2022-05-15T16:07:57.329768Z","iopub.status.idle":"2022-05-15T16:07:57.336490Z","shell.execute_reply.started":"2022-05-15T16:07:57.329719Z","shell.execute_reply":"2022-05-15T16:07:57.335257Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"['Happy', 'Fear', 'Disgust', 'Surprise', 'Sad', 'Angry', 'Neutral']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Here, we see a list of labels as directory names. Let's remind the different ways to connect the pictures and their metadata such as labels.\n\n## 1.3 Ways of storing the data and metadata\n\nMany computer vision data sets are organized in three ways.\n\n<dt>File trees</dt>\n<df>The picture files are grouped by meaningfull directories. We may find their label and other information about them with the directory they belong to. Finding them is done by exploring what is called a ***file tree***</df>\n\n<dt>Metadata</dt>\n<df>The picture files are not well separated in the file tree, or might even be stored on different repositories on the web. To retrieve them and the corresponding labels, we have a Metadata file (can be a JSON or CSV file) containing the file list, file paths and other information like the labels</df>\n\n<dt>File names</dt>\n<df>As we saw in the PETS dataset, where cats were lowercases and dogs were uppercases file names, some pictures of some datasets contain their label inside the filename itself</df>","metadata":{}},{"cell_type":"markdown","source":"## 1.4 Class analysis from file tree\nHere, the labels are conveniently stored as directory names, so they are easy to extract.","metadata":{}},{"cell_type":"code","source":"print(f'total number of labels: {len(labels)}')","metadata":{"execution":{"iopub.status.busy":"2022-05-15T13:58:18.376376Z","iopub.execute_input":"2022-05-15T13:58:18.377632Z","iopub.status.idle":"2022-05-15T13:58:18.382947Z","shell.execute_reply.started":"2022-05-15T13:58:18.377568Z","shell.execute_reply":"2022-05-15T13:58:18.382180Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"total number of labels: 7\n","output_type":"stream"}]},{"cell_type":"code","source":"instancesPerClassDict = {}\n\nfilepathDictionary = {}\n\nfor label in os.listdir(EMOTIONS_IMAGES_DIR):\n    \n    # This gives us a label for each bit of code\n    imagesDirectory = os.path.join(EMOTIONS_IMAGES_DIR,label)\n    \n    # This code gives a list of all the images in the directory\n    \n    images = os.listdir(imagesDirectory)\n    for fileName in images:\n        \n        imagePath = os.path.join(imagesDirectory,fileName) # makes a longer path\n        # use imagePath to do something\n            \n    instancesPerClassDict[label] = len(images)\n    \nfor key, value in instancesPerClassDict.items():\n    print(f'Class: ({key}) contains {value} instances')\n","metadata":{"execution":{"iopub.status.busy":"2022-05-15T16:08:33.732184Z","iopub.execute_input":"2022-05-15T16:08:33.732457Z","iopub.status.idle":"2022-05-15T16:08:33.741887Z","shell.execute_reply.started":"2022-05-15T16:08:33.732430Z","shell.execute_reply":"2022-05-15T16:08:33.741002Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Class: (Happy) contains 51 instances\nClass: (Fear) contains 51 instances\nClass: (Disgust) contains 51 instances\nClass: (Surprise) contains 101 instances\nClass: (Sad) contains 51 instances\nClass: (Angry) contains 51 instances\nClass: (Neutral) contains 51 instances\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now, we have our data in a format that it can be put into a fastAI model to give us more information.","metadata":{}},{"cell_type":"markdown","source":"## 1.5 Class analysis from Metadata File","metadata":{}},{"cell_type":"markdown","source":"In some cases, the files may be located on a cloud server, or put together into one large directory. This means that the information is not organized with a file tree, but rather in metadata.\n\nThis metadata can come as a CSV or a JSON File.","metadata":{}},{"cell_type":"markdown","source":"Here, we work with the google landmarks image dataset, it contains lots of information on different photos that users have uploaded along with different kinds of metadata.\n\nHowever, the  dataset is quite large, so it makes more sense to have an individual metadata files. It can allows us to donwload only the pictures we want.\n\nThese metadata files contain both the labels and the file paths that we need.\n\n***Identifying the x and the y***\nThe features (x) in this case are the images ; labels (y) in this case are the landmark IDs.","metadata":{"tags":[]}},{"cell_type":"code","source":"googleLandmarksPath = os.path.join(DATA_DIR,'CV','landmarks')\nos.path.isdir(googleLandmarksPath)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T16:08:37.559420Z","iopub.execute_input":"2022-05-15T16:08:37.559704Z","iopub.status.idle":"2022-05-15T16:08:37.566233Z","shell.execute_reply.started":"2022-05-15T16:08:37.559670Z","shell.execute_reply":"2022-05-15T16:08:37.565493Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# This snippet can be useful for handling many csv files\n# This code does not work on Windows environment\n\nfor dirname, _, filenames in os.walk(googleLandmarksPath):\n    for filename in filenames:  # loops through all the files in the directories\n        print(filename) # Gives a file name, without the complete file path\n        filepath = os.path.join(dirname,filename) # Completes the file path\n        filename = filename.split('.')[0] # Removes the file extension for naming\n        if os.path.isfile(filepath):\n            ExecString = f\"{filename} = pd.read_csv('{filepath}')\"\n            print(ExecString)\n            exec(ExecString)\n        else:\n            print(f'there is a problem with {filepath}')","metadata":{"execution":{"iopub.status.busy":"2022-05-15T16:08:39.110191Z","iopub.execute_input":"2022-05-15T16:08:39.110480Z","iopub.status.idle":"2022-05-15T16:08:39.176865Z","shell.execute_reply.started":"2022-05-15T16:08:39.110448Z","shell.execute_reply":"2022-05-15T16:08:39.175993Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"retrieval_solution.csv\nretrieval_solution = pd.read_csv('/kaggle/working/DL201/data/CV/landmarks/retrieval_solution.csv')\nrecognition_solution.csv\nrecognition_solution = pd.read_csv('/kaggle/working/DL201/data/CV/landmarks/recognition_solution.csv')\ntrain.csv\ntrain = pd.read_csv('/kaggle/working/DL201/data/CV/landmarks/train.csv')\ntest.csv\ntest = pd.read_csv('/kaggle/working/DL201/data/CV/landmarks/test.csv')\nboxes_split2.csv\nboxes_split2 = pd.read_csv('/kaggle/working/DL201/data/CV/landmarks/boxes_split2.csv')\nindex.csv\nindex = pd.read_csv('/kaggle/working/DL201/data/CV/landmarks/index.csv')\nlandmarksample.csv\nlandmarksample = pd.read_csv('/kaggle/working/DL201/data/CV/landmarks/landmarksample.csv')\nboxes_split1.csv\nboxes_split1 = pd.read_csv('/kaggle/working/DL201/data/CV/landmarks/boxes_split1.csv')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now that we loaded all the metadata files. We will look at their name to know what they represent, and check their shape to know rapidly the number of features and sample they describe. If the name of the file is not clear enough, we can sometimes get some information by observing the relationships between the shapes of two files.","metadata":{}},{"cell_type":"code","source":"# Train\nprint(train.shape)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-15T16:08:48.369570Z","iopub.execute_input":"2022-05-15T16:08:48.370146Z","iopub.status.idle":"2022-05-15T16:08:48.393289Z","shell.execute_reply.started":"2022-05-15T16:08:48.370097Z","shell.execute_reply":"2022-05-15T16:08:48.392360Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"(2000, 4)\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                id  \\\n0     1221181  638a7921e893de63   \n1      393869  ec108f2c67d28082   \n2      833769  8b8d9329e3fccd0a   \n3      400348  cf6b02c51de2f6a1   \n4      749128  c798f1620d54d8c1   \n\n                                                 url landmark_id  \n0  http://lh6.ggpht.com/-okvU3kjsKWQ/TjnHZCjgLdI/...        3918  \n1  https://lh4.googleusercontent.com/-ghGLWnz_5Nw...        4786  \n2  https://lh4.googleusercontent.com/-49VHsrW3voo...        6090  \n3  http://lh6.ggpht.com/-6Afsd7_E_ck/RsSKkV5EnnI/...        6051  \n4                                               None        None  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>id</th>\n      <th>url</th>\n      <th>landmark_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1221181</td>\n      <td>638a7921e893de63</td>\n      <td>http://lh6.ggpht.com/-okvU3kjsKWQ/TjnHZCjgLdI/...</td>\n      <td>3918</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>393869</td>\n      <td>ec108f2c67d28082</td>\n      <td>https://lh4.googleusercontent.com/-ghGLWnz_5Nw...</td>\n      <td>4786</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>833769</td>\n      <td>8b8d9329e3fccd0a</td>\n      <td>https://lh4.googleusercontent.com/-49VHsrW3voo...</td>\n      <td>6090</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>400348</td>\n      <td>cf6b02c51de2f6a1</td>\n      <td>http://lh6.ggpht.com/-6Afsd7_E_ck/RsSKkV5EnnI/...</td>\n      <td>6051</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>749128</td>\n      <td>c798f1620d54d8c1</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train.drop(['Unnamed: 0'],axis=1,inplace=True)\nprint(train.shape)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-15T16:08:48.507148Z","iopub.execute_input":"2022-05-15T16:08:48.507444Z","iopub.status.idle":"2022-05-15T16:08:48.528327Z","shell.execute_reply.started":"2022-05-15T16:08:48.507401Z","shell.execute_reply":"2022-05-15T16:08:48.527252Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"(2000, 3)\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                 id                                                url  \\\n0  638a7921e893de63  http://lh6.ggpht.com/-okvU3kjsKWQ/TjnHZCjgLdI/...   \n1  ec108f2c67d28082  https://lh4.googleusercontent.com/-ghGLWnz_5Nw...   \n2  8b8d9329e3fccd0a  https://lh4.googleusercontent.com/-49VHsrW3voo...   \n3  cf6b02c51de2f6a1  http://lh6.ggpht.com/-6Afsd7_E_ck/RsSKkV5EnnI/...   \n4  c798f1620d54d8c1                                               None   \n\n  landmark_id  \n0        3918  \n1        4786  \n2        6090  \n3        6051  \n4        None  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>url</th>\n      <th>landmark_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>638a7921e893de63</td>\n      <td>http://lh6.ggpht.com/-okvU3kjsKWQ/TjnHZCjgLdI/...</td>\n      <td>3918</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ec108f2c67d28082</td>\n      <td>https://lh4.googleusercontent.com/-ghGLWnz_5Nw...</td>\n      <td>4786</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8b8d9329e3fccd0a</td>\n      <td>https://lh4.googleusercontent.com/-49VHsrW3voo...</td>\n      <td>6090</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cf6b02c51de2f6a1</td>\n      <td>http://lh6.ggpht.com/-6Afsd7_E_ck/RsSKkV5EnnI/...</td>\n      <td>6051</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>c798f1620d54d8c1</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"In this dataset, the label is the landmark ID\n\nThe file location is a little more tricky because it is stored on google's servers, and needs to be downloaded. ","metadata":{}},{"cell_type":"code","source":"landmarkLabels = train['landmark_id']\nlandmarkURLs = train['url']","metadata":{"execution":{"iopub.status.busy":"2022-05-15T16:08:52.129467Z","iopub.execute_input":"2022-05-15T16:08:52.130283Z","iopub.status.idle":"2022-05-15T16:08:52.137169Z","shell.execute_reply.started":"2022-05-15T16:08:52.130223Z","shell.execute_reply":"2022-05-15T16:08:52.136422Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Although these are different, the principles remain the same as before : we need to know what the image is, and where the image is stored. The label and the file path/url will hold this information.\n\nYou can check another example here : https://www.kaggle.com/piyushrg/computer-vision-av-fastai/notebook","metadata":{}},{"cell_type":"markdown","source":"# Part 2: Exercise\n<hr style=\"border:4px solid gray\"> </hr>","metadata":{}},{"cell_type":"markdown","source":"Let's to the same thing with a computer vision dataset from Kaggle.\nThe first step is to create a notebook based on that data.\nFor it, please go to the dataset page : https://www.kaggle.com/datasets/muratkokludataset/rice-image-dataset/code\nAnd then, click on ***New notebook***\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## Part 3 : Model Training XXX TO REMOVE ???\n<hr style=\"border:4px solid gray\"> </hr>\n","metadata":{}},{"cell_type":"markdown","source":"In order to leverage a fastAI model, the two key pieces of information that we will need to fit the data into the model are the labels, and how the images are stored. \n\nOnce we can do that, we can go head and train a ***preliminary model*** to get quantified information on how to build the final model.","metadata":{}},{"cell_type":"code","source":"# Imports, this cell must be run twice because there are dependencies problems with the preloaded libraries of Kaggle\n!pip install -Uqq fastbook[full]\n\nfrom fastbook import *\nfrom fastai.vision.widgets import *\nsetup_book()","metadata":{"execution":{"iopub.status.busy":"2022-05-15T12:07:15.313382Z","iopub.execute_input":"2022-05-15T12:07:15.315374Z","iopub.status.idle":"2022-05-15T12:07:25.392793Z","shell.execute_reply.started":"2022-05-15T12:07:15.315329Z","shell.execute_reply":"2022-05-15T12:07:25.391521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loading the Data into FastAI Dataloader\n\nOnce we have the paths of each image, we can use a dataloader to preprocess and transform the data automatically for us using FastAI","metadata":{}},{"cell_type":"code","source":"class DataLoaders(GetAttr):\n  def __init__(self, *loaders): self.loaders = loaders\n  def __getitem__(self, i): return self.loaders[i]\n  train,valid = add_props(lambda i, self: self[i])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotions = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=99),\n    get_y=parent_label,\n    item_tfms=Resize(225,225)\n    )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotions = emotions.new(\n    item_tfms=RandomResizedCrop(28, min_scale=0.5),\n    batch_tfms=aug_transforms()\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls = emotions.dataloaders(EMOTIONS_IMAGES_DIR)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 2 : Training the Model","metadata":{}},{"cell_type":"code","source":"learn = cnn_learner(dls, resnet34, metrics=error_rate)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.fine_tune(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 3: Interpreting the Results","metadata":{}},{"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What is the Accuracy of the Model?","metadata":{}},{"cell_type":"code","source":"interp.print_classification_report()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What Does the Confusion Matrix Look Like?","metadata":{}},{"cell_type":"markdown","source":"interp.confusion","metadata":{}},{"cell_type":"code","source":"interp.plot_confusion_matrix()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Do you see any clear patterns in the confusion matrix?","metadata":{}},{"cell_type":"markdown","source":"* Can you think of reasons why it got confused?","metadata":{}},{"cell_type":"markdown","source":"### What are the Most Confused Images?","metadata":{}},{"cell_type":"markdown","source":"* Should any of these Images be removed from the Dataset? \n* Are they Clustered in One or a Few Classes?","metadata":{}},{"cell_type":"markdown","source":"### Are they Distributed across many Classes?","metadata":{}},{"cell_type":"markdown","source":"### Can you find any poor quality images?","metadata":{}},{"cell_type":"markdown","source":"### Can Filters be Applied to Improve the Quality of the Images?","metadata":{}},{"cell_type":"markdown","source":"### Are any Features Blocked or Occluded in these images?","metadata":{}},{"cell_type":"markdown","source":"### Can any Features be Highlighted to Improve the Model's Performance?","metadata":{}},{"cell_type":"markdown","source":"### Where Does this First Run Strengthen your Hypothesis?","metadata":{}},{"cell_type":"markdown","source":"### Have Any Other Weaknesses in the Model been Revealed? ","metadata":{}},{"cell_type":"markdown","source":"### Are There Any Major Class Inbalances that Affect Model Performance?","metadata":{}},{"cell_type":"markdown","source":"* Can These be improved by Image Augmentation? ","metadata":{}},{"cell_type":"markdown","source":"### Can you collect more data in areas where your model is weak?","metadata":{}},{"cell_type":"markdown","source":"* Can you Easily Find More Data by Reverse Image Searching the Most Confused Images?","metadata":{}},{"cell_type":"markdown","source":"### How Could Image Preprocessing Make Features More Apparent to the Model?","metadata":{}}]}