{"cells":[{"cell_type":"markdown","metadata":{},"source":["## ðŸ’» UnpackAI DL201 Bootcamp - Week 3 - CV tasks\n","\n","### ðŸ“• Learning Objectives\n","* Explore working examples os using pre-trained models for common CV tasks\n","* Get tips and insights for the adaptation of examples to the final project.\n","\n","### ðŸ“– Concepts map\n","* image classification\n","* object detection\n","* image segmentation"]},{"cell_type":"markdown","metadata":{},"source":["### Code preparation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Install packages (comment if not required)\n","!pip install -Uqq  ipywidgets fastai fastbook\n","\n","# Import dependencies for all sample AI applications (again, to test the environment)\n","import os\n","import numpy as np\n","import tensorflow as tf\n","import cv2\n","import matplotlib.pyplot as plt\n","import pandas\n","import torch\n","from fastai.vision.all import *\n","from fastai.text.all import *\n","from fastai.collab import *\n","from fastai.tabular.all import *\n","import ipywidgets as widgets\n","from IPython.display import Image\n","import urllib.request\n","import requests\n","from huggingface_hub import from_pretrained_keras"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install --upgrade git+https://github.com/rbrtwlz/fastai_object_detection -qq\n","from fastai_object_detection.all import *"]},{"cell_type":"markdown","metadata":{},"source":["### Image classification example: Dogs vs Cats"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","AI application sample: collect images form the PETS dataset to train a \n","RESNET-based dogs vs cats classifier\n","\"\"\"\n","\n","# Download images, navigate to the folder and display some of the images\n","image_path = untar_data(URLs.PETS)/'images'\n","os.chdir(image_path)\n","filenames = os.listdir('.')\n","\n","def slider_callback(position):\n","    image_object = Image(filename=filenames[position], width=600)\n","    display(image_object)\n","\n","widgets.interact(slider_callback, position=widgets.IntSlider(min=0, max=len(filenames), step=1))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# On this dataset, cat images filenames beggin with an uppercase letter\n","print(filenames[:11])\n","\n","# Define a function that uses that property to select if a filename is a cat\n","def is_cat(filename):\n","    return filename[0].isupper()\n","\n","# Create a dataloader\n","data_loader = ImageDataLoaders.from_name_func(\n","    path=image_path, fnames=get_image_files(image_path), label_func=is_cat, valid_pct=0.2, seed=42,\n","    item_tfms=Resize(224)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Feed data to model and train, train with 1 epoch\n","\"\"\"\n","Note: Usually more epochs are required to achieve a good result\n","but given the quality of the dataset and the model in this case is enough.\n","\"\"\"\n","\n","image_learner = cnn_learner(data_loader, resnet34, metrics=error_rate)\n","image_learner.fine_tune(1)"]},{"cell_type":"markdown","metadata":{},"source":["### Object detection example: Cats vs Dogs \n","source: https://rbrtwlz.github.io/fastai_object_detection/"]},{"cell_type":"markdown","metadata":{},"source":["It comes with a fastai DataLoaders class for object detection, prepared and easy to use models and some metrics to measure generated bounding boxes (mAP). So you can train a model for object detection in the simple fastai way with one of the included Learner classes.\n","\n","All you need is a pandas DataFrame containing the data for each object in the images. In default setting follwing columns are required:\n","\n","For the image, which contains the object(s):\n","\n","    image_id\n","    image_path\n","\n","The object's bounding box:\n","\n","    x_min\n","    y_min\n","    x_max\n","    y_max\n","\n","The object's class/label:\n","\n","    class_name\n","\n","If you want to use a model for instance segementation, following columns are additionally required:\n","\n","    mask_path (path to the binary mask, which represents the object in the image)\n","\n","There are helper functions available, for example for adding the image_path by image_id or to change the bbox format from xywh to x1y1x2y2."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get dataset\n","path, df = CocoData.create(ds_name=\"coco-cats-and-dogs\", cat_list=[\"cat\", \"dog\"], max_images=2000, with_mask=False)"]},{"cell_type":"markdown","metadata":{},"source":["Microsoft COCO dataset contains 328,000 annotated images of 91 object categories, so you can pick the categories you want and download just associated images."]},{"cell_type":"markdown","metadata":{},"source":["Then you can build DataLoaders, using it's from_df factory method"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dls = ObjectDetectionDataLoaders.from_df(df, bs=2, \n","                                         item_tfms=[Resize(800, method=\"pad\", pad_mode=\"zeros\")], \n","                                         batch_tfms=[Normalize.from_stats(*imagenet_stats)])\n","dls.show_batch(figsize=(10,10))"]},{"cell_type":"markdown","metadata":{},"source":["Now you are ready to create your fasterrcnn_learner to train a FasterRCNN model (with resnet50 backbone). To validate your models predictions you can use metrics like mAP_at_IoU60."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["learn = fasterrcnn_learner(dls, fasterrcnn_resnet50, \n","                           opt_func=SGD, lr=0.005, wd=0.0005, train_bn=False,\n","                           metrics=[mAP_at_IoU40, mAP_at_IoU60])\n","learn.lr_find()\n","learn.fit_one_cycle(10, 1e-04)"]},{"cell_type":"markdown","metadata":{},"source":["### Image segementation example: Localize common objects in images\n","Creating a model that can recognize the content of every individual pixel in an image is called *segmentation*. Here is how we can train a segmentation model with fastai, using a subset of the [*Camvid* dataset](http://www0.cs.ucl.ac.uk/staff/G.Brostow/papers/Brostow_2009-PRL.pdf) from the paper \"Semantic Object Classes in Video: A High-Definition Ground Truth Database\" by Gabruel J. Brostow, Julien Fauqueur, and Roberto Cipolla:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["path = untar_data(URLs.CAMVID_TINY)\n","dls = SegmentationDataLoaders.from_label_func(\n","    path, bs=8, fnames = get_image_files(path/\"images\"),\n","    label_func = lambda o: path/'labels'/f'{o.stem}_P{o.suffix}',\n","    codes = np.loadtxt(path/'codes.txt', dtype=str)\n",")\n","\n","learn = unet_learner(dls, resnet34)\n","learn.fine_tune(8)"]},{"cell_type":"markdown","metadata":{},"source":["We can visualize how well it achieved its task, by asking the model to color-code each pixel of an image. As you can see, it nearly perfectly classifies every pixel in every object. For instance, notice that all of the cars are overlaid with the same color and all of the trees are overlaid with the same color (in each pair of images, the lefthand image is the ground truth label and the right is the prediction from the model):"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["learn.show_results(max_n=6, figsize=(12, 15))"]},{"cell_type":"markdown","metadata":{},"source":["### Another image segmentation example"]},{"cell_type":"markdown","metadata":{},"source":["Code Initialization"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T14:37:25.178039Z","iopub.status.busy":"2022-05-30T14:37:25.177342Z","iopub.status.idle":"2022-05-30T14:37:33.655848Z","shell.execute_reply":"2022-05-30T14:37:33.65461Z","shell.execute_reply.started":"2022-05-30T14:37:25.178001Z"},"trusted":true},"outputs":[],"source":["# common part\n","import numpy as np\n","import os\n","import tensorflow as tf\n","import cv2\n","import matplotlib.pyplot as plt\n","\n","# to get the pre-trained model\n","from huggingface_hub import from_pretrained_keras\n","\n","# to re-build the model from scratch\n","from glob import glob\n","from scipy.io import loadmat\n","from tensorflow import keras\n","from tensorflow.keras import layers"]},{"cell_type":"markdown","metadata":{},"source":["Dowload a pre-trained image segmentation model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T14:39:03.576596Z","iopub.status.busy":"2022-05-30T14:39:03.576116Z","iopub.status.idle":"2022-05-30T14:39:23.536343Z","shell.execute_reply":"2022-05-30T14:39:23.535322Z","shell.execute_reply.started":"2022-05-30T14:39:03.576561Z"},"trusted":true},"outputs":[],"source":["model = from_pretrained_keras(\"keras-io/deeplabv3p-resnet50\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T13:16:49.368232Z","iopub.status.busy":"2022-05-30T13:16:49.36786Z","iopub.status.idle":"2022-05-30T13:16:49.373264Z","shell.execute_reply":"2022-05-30T13:16:49.372574Z","shell.execute_reply.started":"2022-05-30T13:16:49.368202Z"},"trusted":true},"outputs":[],"source":["model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T14:48:14.705799Z","iopub.status.busy":"2022-05-30T14:48:14.704587Z","iopub.status.idle":"2022-05-30T14:48:14.714136Z","shell.execute_reply":"2022-05-30T14:48:14.712796Z","shell.execute_reply.started":"2022-05-30T14:48:14.705726Z"},"trusted":true},"outputs":[],"source":["model.summary"]},{"cell_type":"markdown","metadata":{},"source":["Prepare custom functions used to prepare the data, call the model and display its results"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T14:42:01.1612Z","iopub.status.busy":"2022-05-30T14:42:01.160526Z","iopub.status.idle":"2022-05-30T14:42:01.183693Z","shell.execute_reply":"2022-05-30T14:42:01.182758Z","shell.execute_reply.started":"2022-05-30T14:42:01.161152Z"},"trusted":true},"outputs":[],"source":["colormap = np.array([[0,0,0], [31,119,180], [44,160,44], [44, 127, 125], [52, 225, 143],\n","                    [217, 222, 163], [254, 128, 37], [130, 162, 128], [121, 7, 166], [136, 183, 248],\n","                    [85, 1, 76], [22, 23, 62], [159, 50, 15], [101, 93, 152], [252, 229, 92],\n","                    [167, 173, 17], [218, 252, 252], [238, 126, 197], [116, 157, 140], [214, 220, 252]], dtype=np.uint8)\n","\n","img_size = 512\n","                    \n","def read_image(image):\n","    image = tf.convert_to_tensor(image)\n","    image.set_shape([None, None, 3])\n","    image = tf.image.resize(images=image, size=[img_size, img_size])\n","    image = image / 127.5 - 1\n","    return image\n","\n","def infer(model, image_tensor):\n","    predictions = model.predict(np.expand_dims((image_tensor), axis=0))\n","    predictions = np.squeeze(predictions)\n","    predictions = np.argmax(predictions, axis=2)\n","    return predictions\n","\n","def decode_segmentation_masks(mask, colormap, n_classes):\n","    r = np.zeros_like(mask).astype(np.uint8)\n","    g = np.zeros_like(mask).astype(np.uint8)\n","    b = np.zeros_like(mask).astype(np.uint8)\n","    for l in range(0, n_classes):\n","        idx = mask == l\n","        r[idx] = colormap[l, 0]\n","        g[idx] = colormap[l, 1]\n","        b[idx] = colormap[l, 2]\n","    rgb = np.stack([r, g, b], axis=2)\n","    return rgb\n","\n","def get_overlay(image, colored_mask):\n","    image = tf.keras.preprocessing.image.array_to_img(image)\n","    image = np.array(image).astype(np.uint8)\n","    overlay = cv2.addWeighted(image, 0.35, colored_mask, 0.65, 0)\n","    return overlay\n","\n","def segmentation(input_image):\n","    image_tensor = read_image(input_image)\n","    prediction_mask = infer(image_tensor=image_tensor, model=model)\n","    prediction_colormap = decode_segmentation_masks(prediction_mask, colormap, 20)\n","    overlay = get_overlay(image_tensor, prediction_colormap)\n","    return (overlay, prediction_colormap)\n","\n","def plot_samples_matplotlib(display_list, figsize=(5, 3)):\n","    _, axes = plt.subplots(nrows=1, ncols=len(display_list), figsize=figsize)\n","    for i in range(len(display_list)):\n","        if display_list[i].shape[-1] == 3:\n","            axes[i].imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n","        else:\n","            axes[i].imshow(display_list[i])\n","    plt.show()\n","\n","def plot_predictions(images_list, colormap, model):\n","    for image_file in images_list:\n","        image_tensor = read_image(image_file)\n","        prediction_mask = infer(image_tensor=image_tensor, model=model)\n","        prediction_colormap = decode_segmentation_masks(prediction_mask, colormap, 20)\n","        overlay = get_overlay(image_tensor, prediction_colormap)\n","        plot_samples_matplotlib(\n","            [image_tensor, overlay, prediction_colormap], figsize=(18, 14)\n","        )"]},{"cell_type":"markdown","metadata":{},"source":["Prepare an input data to test the model\n","\n","suggested read: this tutorial to insert an image into your Kaggle notebook :\n","https://www.kaggle.com/code/michaelshoemaker/adding-images-from-your-pc/notebook\n","\n","Then, use the following code to check where your picture was put"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T14:37:49.569797Z","iopub.status.busy":"2022-05-30T14:37:49.568823Z","iopub.status.idle":"2022-05-30T14:37:49.589897Z","shell.execute_reply":"2022-05-30T14:37:49.588058Z","shell.execute_reply.started":"2022-05-30T14:37:49.569736Z"},"trusted":true},"outputs":[],"source":["for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T14:38:00.749461Z","iopub.status.busy":"2022-05-30T14:38:00.74908Z","iopub.status.idle":"2022-05-30T14:38:00.760736Z","shell.execute_reply":"2022-05-30T14:38:00.759501Z","shell.execute_reply.started":"2022-05-30T14:38:00.74943Z"},"trusted":true},"outputs":[],"source":["picture_path = \"/kaggle/input/picture1/Three_people.jfif\"\n","os.path.isfile(picture_path)"]},{"cell_type":"markdown","metadata":{},"source":["Observe results"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T14:38:40.116767Z","iopub.status.busy":"2022-05-30T14:38:40.115094Z","iopub.status.idle":"2022-05-30T14:38:40.49214Z","shell.execute_reply":"2022-05-30T14:38:40.487597Z","shell.execute_reply.started":"2022-05-30T14:38:40.116704Z"},"trusted":true},"outputs":[],"source":["img = cv2.cvtColor(cv2.imread(picture_path), cv2.COLOR_BGR2RGB)\n","img_array = np.array(img)\n","plt.imshow(img_array)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T14:38:55.027908Z","iopub.status.busy":"2022-05-30T14:38:55.02741Z","iopub.status.idle":"2022-05-30T14:38:55.034064Z","shell.execute_reply":"2022-05-30T14:38:55.032484Z","shell.execute_reply.started":"2022-05-30T14:38:55.027863Z"},"trusted":true},"outputs":[],"source":["picture_list = [img_array]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T14:42:05.583701Z","iopub.status.busy":"2022-05-30T14:42:05.582469Z","iopub.status.idle":"2022-05-30T14:42:07.147067Z","shell.execute_reply":"2022-05-30T14:42:07.146065Z","shell.execute_reply.started":"2022-05-30T14:42:05.583629Z"},"trusted":true},"outputs":[],"source":["plot_predictions(picture_list, colormap, model)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
