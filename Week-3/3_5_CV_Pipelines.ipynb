{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## ðŸ’» UnpackAI DL201 Bootcamp - Week 3 - Image Segmentation\n\n### ðŸ“• Learning Objectives\n\n* Getting working examples able to achieve a CV task\n* Knowing the existence of low-coding functions to achieve a AI taks in computer vision\n\n### ðŸ“– Concepts map\n\n* image segmentation","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-30T13:14:17.310958Z","iopub.execute_input":"2022-05-30T13:14:17.311309Z","iopub.status.idle":"2022-05-30T13:14:17.323262Z","shell.execute_reply.started":"2022-05-30T13:14:17.31128Z","shell.execute_reply":"2022-05-30T13:14:17.322621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 1. Introduction\n\nGiving working example able to inspire you to build your own AI project\n\nOnce the pipeline works, you can decide to tune it, more and more, little by little, as one would do to transform their car for a speed race.\nSo, you can decide to :\n* fine tune the model or train it from scratch (instead of using pre-trained model)\n* clean the training data before feeding the model","metadata":{}},{"cell_type":"markdown","source":"# Part 2. Code preparation","metadata":{}},{"cell_type":"code","source":"# common part\nimport numpy as np\nimport os\nimport tensorflow as tf\nimport cv2\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-05-30T14:37:25.177342Z","iopub.execute_input":"2022-05-30T14:37:25.178039Z","iopub.status.idle":"2022-05-30T14:37:33.655848Z","shell.execute_reply.started":"2022-05-30T14:37:25.178001Z","shell.execute_reply":"2022-05-30T14:37:33.65461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to get the pre-trained model\nfrom huggingface_hub import from_pretrained_keras","metadata":{"execution":{"iopub.status.busy":"2022-05-30T14:37:25.177342Z","iopub.execute_input":"2022-05-30T14:37:25.178039Z","iopub.status.idle":"2022-05-30T14:37:33.655848Z","shell.execute_reply.started":"2022-05-30T14:37:25.178001Z","shell.execute_reply":"2022-05-30T14:37:33.65461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to re-build the model from scratch\nfrom glob import glob\nfrom scipy.io import loadmat\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","metadata":{"execution":{"iopub.status.busy":"2022-05-30T14:37:25.177342Z","iopub.execute_input":"2022-05-30T14:37:25.178039Z","iopub.status.idle":"2022-05-30T14:37:33.655848Z","shell.execute_reply.started":"2022-05-30T14:37:25.178001Z","shell.execute_reply":"2022-05-30T14:37:33.65461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 3. Using a pre-trained image segmentation model","metadata":{}},{"cell_type":"markdown","source":"## Download the model","metadata":{}},{"cell_type":"code","source":"model = from_pretrained_keras(\"keras-io/deeplabv3p-resnet50\")","metadata":{"execution":{"iopub.status.busy":"2022-05-30T14:39:03.576116Z","iopub.execute_input":"2022-05-30T14:39:03.576596Z","iopub.status.idle":"2022-05-30T14:39:23.536343Z","shell.execute_reply.started":"2022-05-30T14:39:03.576561Z","shell.execute_reply":"2022-05-30T14:39:23.535322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2022-05-30T13:16:49.36786Z","iopub.execute_input":"2022-05-30T13:16:49.368232Z","iopub.status.idle":"2022-05-30T13:16:49.373264Z","shell.execute_reply.started":"2022-05-30T13:16:49.368202Z","shell.execute_reply":"2022-05-30T13:16:49.372574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary","metadata":{"execution":{"iopub.status.busy":"2022-05-30T14:48:14.704587Z","iopub.execute_input":"2022-05-30T14:48:14.705799Z","iopub.status.idle":"2022-05-30T14:48:14.714136Z","shell.execute_reply.started":"2022-05-30T14:48:14.705726Z","shell.execute_reply":"2022-05-30T14:48:14.712796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare the functions used to prepare the data, call the model and display its results","metadata":{}},{"cell_type":"code","source":"colormap = np.array([[0,0,0], [31,119,180], [44,160,44], [44, 127, 125], [52, 225, 143],\n                    [217, 222, 163], [254, 128, 37], [130, 162, 128], [121, 7, 166], [136, 183, 248],\n                    [85, 1, 76], [22, 23, 62], [159, 50, 15], [101, 93, 152], [252, 229, 92],\n                    [167, 173, 17], [218, 252, 252], [238, 126, 197], [116, 157, 140], [214, 220, 252]], dtype=np.uint8)\n\nimg_size = 512\n                    \ndef read_image(image):\n    image = tf.convert_to_tensor(image)\n    image.set_shape([None, None, 3])\n    image = tf.image.resize(images=image, size=[img_size, img_size])\n    image = image / 127.5 - 1\n    return image\n\ndef infer(model, image_tensor):\n    predictions = model.predict(np.expand_dims((image_tensor), axis=0))\n    predictions = np.squeeze(predictions)\n    predictions = np.argmax(predictions, axis=2)\n    return predictions\n\ndef decode_segmentation_masks(mask, colormap, n_classes):\n    r = np.zeros_like(mask).astype(np.uint8)\n    g = np.zeros_like(mask).astype(np.uint8)\n    b = np.zeros_like(mask).astype(np.uint8)\n    for l in range(0, n_classes):\n        idx = mask == l\n        r[idx] = colormap[l, 0]\n        g[idx] = colormap[l, 1]\n        b[idx] = colormap[l, 2]\n    rgb = np.stack([r, g, b], axis=2)\n    return rgb\n\ndef get_overlay(image, colored_mask):\n    image = tf.keras.preprocessing.image.array_to_img(image)\n    image = np.array(image).astype(np.uint8)\n    overlay = cv2.addWeighted(image, 0.35, colored_mask, 0.65, 0)\n    return overlay\n\ndef segmentation(input_image):\n    image_tensor = read_image(input_image)\n    prediction_mask = infer(image_tensor=image_tensor, model=model)\n    prediction_colormap = decode_segmentation_masks(prediction_mask, colormap, 20)\n    overlay = get_overlay(image_tensor, prediction_colormap)\n    return (overlay, prediction_colormap)\n\ndef plot_samples_matplotlib(display_list, figsize=(5, 3)):\n    _, axes = plt.subplots(nrows=1, ncols=len(display_list), figsize=figsize)\n    for i in range(len(display_list)):\n        if display_list[i].shape[-1] == 3:\n            axes[i].imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n        else:\n            axes[i].imshow(display_list[i])\n    plt.show()\n\ndef plot_predictions(images_list, colormap, model):\n    for image_file in images_list:\n        image_tensor = read_image(image_file)\n        prediction_mask = infer(image_tensor=image_tensor, model=model)\n        prediction_colormap = decode_segmentation_masks(prediction_mask, colormap, 20)\n        overlay = get_overlay(image_tensor, prediction_colormap)\n        plot_samples_matplotlib(\n            [image_tensor, overlay, prediction_colormap], figsize=(18, 14)\n        )","metadata":{"execution":{"iopub.status.busy":"2022-05-30T14:42:01.160526Z","iopub.execute_input":"2022-05-30T14:42:01.1612Z","iopub.status.idle":"2022-05-30T14:42:01.183693Z","shell.execute_reply.started":"2022-05-30T14:42:01.161152Z","shell.execute_reply":"2022-05-30T14:42:01.182758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare an input data to test the model\n\n### First, read this tutorial to insert an image into your Kaggle notebook :\nhttps://www.kaggle.com/code/michaelshoemaker/adding-images-from-your-pc/notebook\n\n### Then, use the following code to check where your picture was put","metadata":{}},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-05-30T14:37:49.568823Z","iopub.execute_input":"2022-05-30T14:37:49.569797Z","iopub.status.idle":"2022-05-30T14:37:49.589897Z","shell.execute_reply.started":"2022-05-30T14:37:49.569736Z","shell.execute_reply":"2022-05-30T14:37:49.588058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"picture_path = \"/kaggle/input/picture1/Three_people.jfif\"\nos.path.isfile(picture_path)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T14:38:00.74908Z","iopub.execute_input":"2022-05-30T14:38:00.749461Z","iopub.status.idle":"2022-05-30T14:38:00.760736Z","shell.execute_reply.started":"2022-05-30T14:38:00.74943Z","shell.execute_reply":"2022-05-30T14:38:00.759501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare the picture list to feed to the function tf.convert_to_tensor()\n\nSources : geeksforgeeks.org/python-tensorflow-convert_to_tensor/","metadata":{}},{"cell_type":"code","source":"img = cv2.cvtColor(cv2.imread(picture_path), cv2.COLOR_BGR2RGB)\nimg_array = np.array(img)\nplt.imshow(img_array)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T14:38:40.115094Z","iopub.execute_input":"2022-05-30T14:38:40.116767Z","iopub.status.idle":"2022-05-30T14:38:40.49214Z","shell.execute_reply.started":"2022-05-30T14:38:40.116704Z","shell.execute_reply":"2022-05-30T14:38:40.487597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"picture_list = [img_array]","metadata":{"execution":{"iopub.status.busy":"2022-05-30T14:38:55.02741Z","iopub.execute_input":"2022-05-30T14:38:55.027908Z","iopub.status.idle":"2022-05-30T14:38:55.034064Z","shell.execute_reply.started":"2022-05-30T14:38:55.027863Z","shell.execute_reply":"2022-05-30T14:38:55.032484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_predictions(picture_list, colormap, model)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T14:42:05.582469Z","iopub.execute_input":"2022-05-30T14:42:05.583701Z","iopub.status.idle":"2022-05-30T14:42:07.147067Z","shell.execute_reply.started":"2022-05-30T14:42:05.583629Z","shell.execute_reply":"2022-05-30T14:42:07.146065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 4. How to re-build this model from scratch and train it with our own data\n\nSources : https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/deeplabv3_plus.ipynb#scrollTo=4qQr7_4y-2uZ","metadata":{}},{"cell_type":"code","source":"from glob import glob\nfrom scipy.io import loadmat\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMAGE_SIZE = 512\nBATCH_SIZE = 4\nNUM_CLASSES = 20\nDATA_DIR = \"XXXXX\"\nNUM_TRAIN_IMAGES = 1000\nNUM_VAL_IMAGES = 50\n\ntrain_images = sorted(glob(os.path.join(DATA_DIR, \"Images/*\")))[:NUM_TRAIN_IMAGES]\ntrain_masks = sorted(glob(os.path.join(DATA_DIR, \"Category_ids/*\")))[:NUM_TRAIN_IMAGES]\nval_images = sorted(glob(os.path.join(DATA_DIR, \"Images/*\")))[\n    NUM_TRAIN_IMAGES : NUM_VAL_IMAGES + NUM_TRAIN_IMAGES\n]\nval_masks = sorted(glob(os.path.join(DATA_DIR, \"Category_ids/*\")))[\n    NUM_TRAIN_IMAGES : NUM_VAL_IMAGES + NUM_TRAIN_IMAGES\n]\n\n\ndef read_image(image_path, mask=False):\n    image = tf.io.read_file(image_path)\n    if mask:\n        image = tf.image.decode_png(image, channels=1)\n        image.set_shape([None, None, 1])\n        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n    else:\n        image = tf.image.decode_png(image, channels=3)\n        image.set_shape([None, None, 3])\n        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n        image = image / 127.5 - 1\n    return image\n\n\ndef load_data(image_list, mask_list):\n    image = read_image(image_list)\n    mask = read_image(mask_list, mask=True)\n    return image, mask\n\n\ndef data_generator(image_list, mask_list):\n    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))\n    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n    return dataset\n\n\ntrain_dataset = data_generator(train_images, train_masks)\nval_dataset = data_generator(val_images, val_masks)\n\nprint(\"Train Dataset:\", train_dataset)\nprint(\"Val Dataset:\", val_dataset)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convolution_block(\n    block_input,\n    num_filters=256,\n    kernel_size=3,\n    dilation_rate=1,\n    padding=\"same\",\n    use_bias=False,\n):\n    x = layers.Conv2D(\n        num_filters,\n        kernel_size=kernel_size,\n        dilation_rate=dilation_rate,\n        padding=\"same\",\n        use_bias=use_bias,\n        kernel_initializer=keras.initializers.HeNormal(),\n    )(block_input)\n    x = layers.BatchNormalization()(x)\n    return tf.nn.relu(x)\n\n\ndef DilatedSpatialPyramidPooling(dspp_input):\n    dims = dspp_input.shape\n    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n    x = convolution_block(x, kernel_size=1, use_bias=True)\n    out_pool = layers.UpSampling2D(\n        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation=\"bilinear\",\n    )(x)\n\n    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n\n    x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n    output = convolution_block(x, kernel_size=1)\n    return output\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def DeeplabV3Plus(image_size, num_classes):\n    model_input = keras.Input(shape=(image_size, image_size, 3))\n    resnet50 = keras.applications.ResNet50(\n        weights=\"imagenet\", include_top=False, input_tensor=model_input\n    )\n    x = resnet50.get_layer(\"conv4_block6_2_relu\").output\n    x = DilatedSpatialPyramidPooling(x)\n\n    input_a = layers.UpSampling2D(\n        size=(image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]),\n        interpolation=\"bilinear\",\n    )(x)\n    input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n\n    x = layers.Concatenate(axis=-1)([input_a, input_b])\n    x = convolution_block(x)\n    x = convolution_block(x)\n    x = layers.UpSampling2D(\n        size=(image_size // x.shape[1], image_size // x.shape[2]),\n        interpolation=\"bilinear\",\n    )(x)\n    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding=\"same\")(x)\n    return keras.Model(inputs=model_input, outputs=model_output)\n\n\nmodel = DeeplabV3Plus(image_size=IMAGE_SIZE, num_classes=NUM_CLASSES)\nmodel.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n    loss=loss,\n    metrics=[\"accuracy\"],\n)\n\nhistory = model.fit(train_dataset, validation_data=val_dataset, epochs=25)\n\nplt.plot(history.history[\"loss\"])\nplt.title(\"Training Loss\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")\nplt.show()\n\nplt.plot(history.history[\"accuracy\"])\nplt.title(\"Training Accuracy\")\nplt.ylabel(\"accuracy\")\nplt.xlabel(\"epoch\")\nplt.show()\n\nplt.plot(history.history[\"val_loss\"])\nplt.title(\"Validation Loss\")\nplt.ylabel(\"val_loss\")\nplt.xlabel(\"epoch\")\nplt.show()\n\nplt.plot(history.history[\"val_accuracy\"])\nplt.title(\"Validation Accuracy\")\nplt.ylabel(\"val_accuracy\")\nplt.xlabel(\"epoch\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 5. How to fine tune an image segmentation model\n\nSource : https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n\nSource : https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/torchvision_finetuning_instance_segmentation.ipynb#scrollTo=DBIoe_tHTQgV","metadata":{}}]}